{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import tqdm\n",
    "import yaml\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from scipy.stats import ttest_1samp, t, chi2_contingency\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import betaln\n",
    "from scipy.stats import beta\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "# pio.renderers.default = \"notebook\"\n",
    "# pio.renderers.default = \"notebook_connected\"\n",
    "pio.renderers.default = \"iframe\"\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.feature_extraction import data_quality_check\n",
    "from src.utils import get_root_directory, float_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get root directory of the project\n",
    "root_dir = get_root_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximize Column Display \n",
    "pd.set_option('display.max_colwidth', None)     # Display all content within each cell without truncation\n",
    "pd.set_option('display.max_columns', None)      # Display all columns\n",
    "pd.set_option('display.width', None)            # Display entire width of DataFrame is displayed\n",
    "\n",
    "pd.set_option('display.max_rows', None)         # Display all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CPU usage: 6.2%\n",
      "CPU usage per core: [6.1, 6.1, 3.0, 4.1, 22.2, 7.0, 4.0, 5.1, 5.1, 4.0, 12.0, 7.1]\n",
      "Total CPU cores: 12\n"
     ]
    }
   ],
   "source": [
    "# Get the current CPU usage as a percentage\n",
    "cpu_usage = psutil.cpu_percent(interval=1)  # Interval of 1 second\n",
    "print(f\"Current CPU usage: {cpu_usage}%\")\n",
    "\n",
    "# Get the per-core usage\n",
    "cpu_per_core = psutil.cpu_percent(interval=1, percpu=True)\n",
    "print(f\"CPU usage per core: {cpu_per_core}\")\n",
    "\n",
    "# Get the total number of cores\n",
    "cpu_cores = psutil.cpu_count()\n",
    "print(f\"Total CPU cores: {cpu_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDOT D5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "signal_ids = [\n",
    "    \"1285\", \"1290\",\n",
    "    \"1300\", \"1315\", \"1325\", \"1330\", \n",
    "    \"1455\", \"1470\", \"1490\",\n",
    "    \"1500\", \"1555\",\n",
    "    \"1707\", \"1725\", \"1790\", \"1795\", \n",
    "    \"1960\",\n",
    "    \"2055\", \n",
    "    \"2485\", \n",
    "    \"2665\", \n",
    "    # \"D5I-3000\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_data(dirpath: str, signal_id: str):\n",
    "    # Cycle-level SPaT\n",
    "    filepaths = f\"{dirpath}/{signal_id}/*\"\n",
    "    filepaths = [filepath for filepath in glob.glob(filepaths)]\n",
    "\n",
    "    data = []\n",
    "    for filepath in filepaths:\n",
    "        data.append(pd.read_pickle(filepath))\n",
    "        \n",
    "    df_id = pd.concat(data, axis=0, ignore_index=True)\n",
    "\n",
    "    return df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, func=np.mean, n_iterations=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate bootstrapped confidence intervals with flexibility for mean or median.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Input data for bootstrapping. NaN values will be ignored.\n",
    "    func : callable\n",
    "        Function to apply to each bootstrap sample (e.g., np.mean, np.median).\n",
    "    n_iterations : int\n",
    "        Number of bootstrap iterations (default: 1000).\n",
    "    alpha : float\n",
    "        Significance level for confidence intervals (default: 0.05).\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Lower and upper bounds of the confidence interval.\n",
    "        Returns (np.nan, np.nan) if the input data is entirely NaN.\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    data = np.array(data)\n",
    "    data = data[~np.isnan(data)]\n",
    "    \n",
    "    # Handle case where all data is NaN\n",
    "    if len(data) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Perform bootstrapping\n",
    "    bootstrap_samples = [func(np.random.choice(data, size=len(data), replace=True)) for _ in range(n_iterations)]\n",
    "    lower = np.percentile(bootstrap_samples, alpha / 2 * 100)\n",
    "    upper = np.percentile(bootstrap_samples, (1 - alpha / 2) * 100)\n",
    "    \n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Delete unwanted file\n",
    "# dirpath = Path(\"../data/production/atspm/fdot_d5/feature_extraction/\")\n",
    "# file_name = \"2024-06-10.pkl\"\n",
    "\n",
    "# # Recursively search for the target file in all folders and subfolders\n",
    "# for filepath in dirpath.rglob(file_name):\n",
    "#     print(f\"Deleting: {filepath}\")\n",
    "#     filepath.unlink() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pedestrian Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identification of Critical Hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pedestrian Demand Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_demand_probability(df, A_col, C_col, output_col=\"demandProbability\"):\n",
    "    \"\"\"\n",
    "    Calculate pedestrian demand probability for columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing pedestrian activity and cycle length.\n",
    "    A_col (str): Name of the column for pedestrian activity (average arrivals per hour).\n",
    "    C_col (str): Name of the column for cycle length (seconds).\n",
    "    output_col (str): Name of the column to store pedestrian demand probability.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with an added column for pedestrian demand probability.\n",
    "    \"\"\"\n",
    "    def calculate_pedestrian_demand(A, C):\n",
    "        if C <= 0:\n",
    "            return None  # Handle invalid cycle length\n",
    "        return 1 - math.exp(-(A * C) / 3600)\n",
    "    \n",
    "    # Apply the calculation row-wise\n",
    "    df[output_col] = df.apply(\n",
    "        lambda row: calculate_pedestrian_demand(row[A_col], row[C_col]), axis=1\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_ci_with_demand_probability(signal_id):\n",
    "    # Cycle-level SPaT\n",
    "    df_spat_id = load_data(\n",
    "        dirpath=\"../data/production/atspm/fdot_d5/feature_extraction/feature/cycle/vehicle_signal/spat\",\n",
    "        signal_id=signal_id\n",
    "    )\n",
    "    columns = [column for column in df_spat_id.columns if not any(k in column for k in [\"yellow\", \"redClearance\", \"red\", \"1\", \"3\", \"5\", \"7\"])]\n",
    "    df_spat_id = df_spat_id[columns]\n",
    "    \n",
    "    duration_columns = [column for column in df_spat_id.columns if \"Duration\" in column]\n",
    "    df_spat_id = (\n",
    "        pd.melt(df_spat_id, \n",
    "                id_vars=[\"signalID\", \"date\", \"cycleNo\", \"cycleBegin\", \"cycleEnd\", \"cycleLength\"], \n",
    "                value_vars=duration_columns, value_name=\"greenDuration\", var_name=\"phaseNo\")\n",
    "    )\n",
    "    \n",
    "    df_spat_id[\"phaseNo\"] = (\n",
    "        df_spat_id[\"phaseNo\"].str.extract(r'Phase(\\d)').astype(int)\n",
    "    )\n",
    "\n",
    "    # Cycle-level pedestrian signal details\n",
    "    df_pedestrian_cycle_profile_id = load_data(\n",
    "        dirpath=\"../data/production/atspm/fdot_d5/feature_extraction/signal_profile/cycle/pedestrian_signal\",\n",
    "        signal_id=signal_id\n",
    "    )\n",
    "    df_pedestrian_cycle_profile_id[\"walkDuration\"] = (\n",
    "        (df_pedestrian_cycle_profile_id[\"pedestrianWalkEnd\"] - df_pedestrian_cycle_profile_id[\"pedestrianWalkBegin\"]).dt.seconds\n",
    "    )\n",
    "    df_pedestrian_cycle_profile_id[\"pedestrianSignalDuration\"] = (\n",
    "        (df_pedestrian_cycle_profile_id[\"pedestrianClearanceEnd\"] - df_pedestrian_cycle_profile_id[\"pedestrianWalkBegin\"]).dt.seconds\n",
    "    )\n",
    "    df_pedestrian_cycle_profile_id = df_pedestrian_cycle_profile_id.drop(\n",
    "        columns=[\"correctSequenceFlag\", \"pedestrianWalkBegin\", \"pedestrianWalkEnd\", \"pedestrianClearanceBegin\", \"pedestrianClearanceEnd\", \"pedestrianDontWalkBegin\"]\n",
    "    )\n",
    "\n",
    "    # Cycle-level pedestrian activity\n",
    "    df_pedestrian_activity_id = load_data(\n",
    "        dirpath=\"../data/production/atspm/fdot_d5/feature_extraction/feature/cycle/pedestrian_traffic/activity\",\n",
    "        signal_id=signal_id\n",
    "    )\n",
    "    columns = [\n",
    "        column for column in df_pedestrian_activity_id.columns if not any(k in column for k in [\"Prev\", \"Curr\", \"45\"])\n",
    "    ]\n",
    "    df_pedestrian_activity_id = df_pedestrian_activity_id[columns]\n",
    "    df_pedestrian_activity_id[\"hour\"] = df_pedestrian_activity_id[\"cycleBegin\"].dt.hour\n",
    "    \n",
    "    activity_columns = [col for col in df_pedestrian_activity_id.columns if \"Activity\" in col]\n",
    "    df_pedestrian_activity_id = df_pedestrian_activity_id.assign(\n",
    "        **{\n",
    "            col: df_pedestrian_activity_id.groupby([\"signalID\", \"date\", \"hour\"])[col].transform(\"sum\")\n",
    "            for col in activity_columns\n",
    "        }\n",
    "    )\n",
    "    df_pedestrian_activity_id = (\n",
    "        pd.melt(df_pedestrian_activity_id, \n",
    "                id_vars=[\"signalID\", \"date\", \"hour\", \"cycleNo\"], \n",
    "                value_vars=activity_columns, value_name=\"pedestrianActivity\", var_name=\"phaseNo\")\n",
    "    )\n",
    "    df_pedestrian_activity_id[\"phaseNo\"] = (\n",
    "        df_pedestrian_activity_id[\"phaseNo\"].str.extract(r'Phase(\\d)').astype(int)\n",
    "    )\n",
    "\n",
    "    # Merge\n",
    "    df_pedestrian_demand_prob_id = (\n",
    "        pd.merge(df_spat_id, df_pedestrian_cycle_profile_id,  \n",
    "                 how=\"left\", on=[\"signalID\", \"date\", \"cycleNo\", \"cycleBegin\", \"cycleEnd\", \"phaseNo\"])\n",
    "    )\n",
    "    df_pedestrian_demand_prob_id[\"hour\"] = df_pedestrian_demand_prob_id[\"cycleBegin\"].dt.hour\n",
    "    \n",
    "    df_pedestrian_demand_prob_id = (\n",
    "        pd.merge(df_pedestrian_demand_prob_id, df_pedestrian_activity_id, \n",
    "                 how=\"left\", on=[\"signalID\", \"date\", \"hour\", \"cycleNo\", \"phaseNo\"])\n",
    "    )\n",
    "    df_pedestrian_demand_prob_id = (\n",
    "        df_pedestrian_demand_prob_id.drop(columns=[\"cycleBegin\", \"cycleEnd\"])\n",
    "    )\n",
    "    df_pedestrian_demand_prob_id[\"cycleNo\"] = df_pedestrian_demand_prob_id[\"cycleNo\"].astype(int)\n",
    "    df_pedestrian_demand_prob_id[\"pedestrianActivity\"] = (\n",
    "        df_pedestrian_demand_prob_id[\"pedestrianActivity\"].apply(lambda x: int(x) if pd.notna(x) else 0)\n",
    "    )\n",
    "    # df_pedestrian_demand_prob_id[\"pedestrianSignalDuration\"] = (\n",
    "    #     df_pedestrian_demand_prob_id.groupby([\"signalID\", \"date\", \"hour\", \"phaseNo\"])[\"pedestrianSignalDuration\"].transform(\"min\")\n",
    "    # )\n",
    "    # CI calculation\n",
    "    df_pedestrian_demand_prob_id = (\n",
    "        calculate_demand_probability(df_pedestrian_demand_prob_id, \n",
    "                                     A_col=\"pedestrianActivity\",     \n",
    "                                     C_col=\"cycleLength\")\n",
    "    )\n",
    "    df_pedestrian_demand_prob_id[\"demandProbability\"] = df_pedestrian_demand_prob_id[\"demandProbability\"].replace(0, np.nan)\n",
    "\n",
    "    \n",
    "    df_pedestrian_demand_prob_id_hourly = (\n",
    "        df_pedestrian_demand_prob_id\n",
    "        .groupby([\"signalID\", \"date\", \"hour\", \"phaseNo\"])\n",
    "        .agg(\n",
    "            greenDurationAvg=(\"greenDuration\", lambda x: np.nan if x.isna().all() else x.mean(skipna=True)),\n",
    "            pedestrianSignalDurationMin=(\"pedestrianSignalDuration\", lambda x: np.nan if x.isna().all() else x.min(skipna=True)),\n",
    "            demandProbabilityAvg=(\"demandProbability\", lambda x: np.nan if x.isna().all() else x.mean(skipna=True)),\n",
    "            # demandProbabilityAvg=(\"demandProbability\", lambda x: np.nan if x.isna().all() else x.max(skipna=True))\n",
    "        )\n",
    "        .reset_index()  \n",
    "    )\n",
    "    \n",
    "    df_pedestrian_demand_prob_id_hourly[\"efficiencyRatioAvg\"] = (\n",
    "        df_pedestrian_demand_prob_id_hourly[\"greenDurationAvg\"] / df_pedestrian_demand_prob_id_hourly[\"pedestrianSignalDurationMin\"]\n",
    "    )\n",
    "\n",
    "    # Group by `phaseNo` and `hour` and calculate bootstrap confidence intervals and mean\n",
    "    pedestrian_demand_probs_id = []\n",
    "    \n",
    "    for (signal_id, hour, phase_no), group in df_pedestrian_demand_prob_id_hourly.groupby([\"signalID\", \"hour\", \"phaseNo\"]):\n",
    "        demand_probs = group[\"demandProbabilityAvg\"].values\n",
    "        efficiency_ratios = group[\"efficiencyRatioAvg\"].values\n",
    "\n",
    "        # Check if demand_probs has valid values (non-NaN)\n",
    "        if len(demand_probs) > 0 and not np.isnan(demand_probs).all():\n",
    "            demand_prob_lower, demand_prob_upper = bootstrap_ci(demand_probs, func=np.mean)\n",
    "            demand_prob_mean = np.nanmean(demand_probs)\n",
    "        else:\n",
    "            # Handle the case where all values are NaN\n",
    "            demand_prob_lower, demand_prob_upper, demand_prob_mean = np.nan, np.nan, np.nan\n",
    "        \n",
    "        # Check if efficiency_ratios has valid values (non-NaN)\n",
    "        if len(efficiency_ratios) > 0 and not np.isnan(efficiency_ratios).all():\n",
    "            efficiency_ratio_lower, efficiency_ratio_upper = bootstrap_ci(efficiency_ratios, func=np.mean)\n",
    "            efficiency_ratio_mean = np.nanmean(efficiency_ratios)\n",
    "        else:\n",
    "            # Handle the case where all values are NaN\n",
    "            efficiency_ratio_lower, efficiency_ratio_upper, efficiency_ratio_mean = np.nan, np.nan, np.nan\n",
    "    \n",
    "        # Append results to the list\n",
    "        pedestrian_demand_probs_id.append(\n",
    "            {\n",
    "                \"signalID\": signal_id, \"hour\": hour, \"phaseNo\": phase_no,\n",
    "                \"demandProbabilityAvg\": demand_prob_mean,\n",
    "                \"demandProbabilityAvgLower\": demand_prob_lower,\n",
    "                \"demandProbabilityAvgUpper\": demand_prob_upper,\n",
    "                \"efficiencyRatioAvg\": efficiency_ratio_mean,\n",
    "                \"efficiencyRatioAvgLower\": efficiency_ratio_lower,\n",
    "                \"efficiencyRatioAvgUpper\": efficiency_ratio_upper\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Create a results DataFrame\n",
    "    df_pedestrian_demand_prob_id_hourly = pd.DataFrame(pedestrian_demand_probs_id)\n",
    "\n",
    "    return df_pedestrian_demand_prob_id, df_pedestrian_demand_prob_id_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# _, df_pedestrian_demand_prob_id_hourly = calculate_ci_with_demand_probability(signal_id=\"1500\")\n",
    "\n",
    "# phase_nos = df_pedestrian_demand_prob_id_hourly[\"phaseNo\"].unique()\n",
    "\n",
    "# # Create a 2x2 grid for subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2, cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase_no}\" for phase_no in phase_nos],\n",
    "#     shared_xaxes=False, shared_yaxes=False,\n",
    "#     vertical_spacing=0.15\n",
    "# )\n",
    "\n",
    "# # Iterate over phases and add traces\n",
    "# row_col_mapping = [(1, 1), (1, 2), (2, 1), (2, 2)]  # Map phases to subplot positions\n",
    "# for idx, (phase_no, (row, col)) in enumerate(zip(phase_nos, row_col_mapping)):\n",
    "#     proc_df_pedestrian_demand_prob_id_hourly = df_pedestrian_demand_prob_id_hourly[df_pedestrian_demand_prob_id_hourly[\"phaseNo\"] == phase_no]\n",
    "    \n",
    "#     # Calculate error bars\n",
    "#     yerr_lower = (\n",
    "#         proc_df_pedestrian_demand_prob_id_hourly[\"demandProbabilityAvg\"] - proc_df_pedestrian_demand_prob_id_hourly[\"demandProbabilityAvgLower\"]\n",
    "#     )\n",
    "#     yerr_upper = (\n",
    "#         proc_df_pedestrian_demand_prob_id_hourly[\"demandProbabilityAvgUpper\"] - proc_df_pedestrian_demand_prob_id_hourly[\"demandProbabilityAvg\"]\n",
    "#     )\n",
    "\n",
    "#     # Add scatter plot with error bars\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=proc_df_pedestrian_demand_prob_id_hourly[\"hour\"],\n",
    "#             y=proc_df_pedestrian_demand_prob_id_hourly[\"demandProbabilityAvg\"],\n",
    "#             error_y=dict(\n",
    "#                 type=\"data\",\n",
    "#                 symmetric=False,\n",
    "#                 array=yerr_upper,\n",
    "#                 arrayminus=yerr_lower,\n",
    "#                 color=\"blue\",\n",
    "#                 thickness=1.5,\n",
    "#                 width=3,\n",
    "#             ),\n",
    "#             mode=\"markers+lines\",\n",
    "#             marker=dict(size=8),\n",
    "#             name=f\"Phase {phase_no}\",\n",
    "#         ),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "#     # Add a horizontal threshold line\n",
    "#     fig.add_shape(\n",
    "#         type=\"line\",\n",
    "#         x0=0,\n",
    "#         x1=23,\n",
    "#         y0=0.6,\n",
    "#         y1=0.6,\n",
    "#         line=dict(color=\"red\", dash=\"dash\"),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "#     # Add a horizontal threshold line\n",
    "#     fig.add_shape(\n",
    "#         type=\"line\",\n",
    "#         x0=0,\n",
    "#         x1=23,\n",
    "#         y0=0.4,\n",
    "#         y1=0.4,\n",
    "#         line=dict(color=\"orange\", dash=\"dash\"),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     # title=\"Proportion of Cycles Recommended with PR by Hour with 95% Confidence Intervals\",\n",
    "#     height=800,\n",
    "#     width=1400,\n",
    "#     showlegend=False,\n",
    "#     xaxis_title=\"Hour of Day\",\n",
    "#     yaxis_title=\"Probability of Pedestrian Demand\",\n",
    "#     yaxis=dict(range=[0, 1]),  # Set y-axis range\n",
    "#     font=dict(size=14)\n",
    "# )\n",
    "\n",
    "# # Update axis labels for shared x/y axes\n",
    "# fig.update_xaxes(\n",
    "#     title_text=\"Hour of Day\",\n",
    "#     # dtick=1,\n",
    "#     # tickvals=list(range(24)),\n",
    "#     # ticktext=[f\"{hour}:00\" for hour in range(24)],\n",
    "#     # tickformat=\"%H:00\",\n",
    "#     # tickangle=-45,\n",
    "#     # row=2, col=1,  # Shared x-axis title position\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Update y-axes for shared configuration\n",
    "# fig.update_yaxes(\n",
    "#     title_text=\"Probability of Pedestrian Demand\", \n",
    "#     range=[0, 0.75],  # Set y-axis range for all subplots\n",
    "#     tickvals=np.linspace(0, 1, 11),  # Tick values from 0 to 1 with step 0.1\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Show plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# _, df_pedestrian_demand_prob_id_hourly = calculate_ci_with_demand_probability(signal_id=\"1500\")\n",
    "\n",
    "# phase_nos = df_pedestrian_demand_prob_id_hourly[\"phaseNo\"].unique()\n",
    "\n",
    "# # Create a 2x2 grid for subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2, cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase_no}\" for phase_no in phase_nos],\n",
    "#     shared_xaxes=False, shared_yaxes=False,\n",
    "#     vertical_spacing=0.15\n",
    "# )\n",
    "\n",
    "# # Iterate over phases and add traces\n",
    "# row_col_mapping = [(1, 1), (1, 2), (2, 1), (2, 2)]  # Map phases to subplot positions\n",
    "# for idx, (phase_no, (row, col)) in enumerate(zip(phase_nos, row_col_mapping)):\n",
    "#     proc_df_pedestrian_demand_prob_id_hourly = df_pedestrian_demand_prob_id_hourly[df_pedestrian_demand_prob_id_hourly[\"phaseNo\"] == phase_no]\n",
    "    \n",
    "#     # Calculate error bars\n",
    "#     yerr_lower = (\n",
    "#         proc_df_pedestrian_demand_prob_id_hourly[\"efficiencyRatioAvg\"] - proc_df_pedestrian_demand_prob_id_hourly[\"efficiencyRatioAvgLower\"]\n",
    "#     )\n",
    "#     yerr_upper = (\n",
    "#         proc_df_pedestrian_demand_prob_id_hourly[\"efficiencyRatioAvgUpper\"] - proc_df_pedestrian_demand_prob_id_hourly[\"efficiencyRatioAvg\"]\n",
    "#     )\n",
    "\n",
    "#     # Add scatter plot with error bars\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=proc_df_pedestrian_demand_prob_id_hourly[\"hour\"],\n",
    "#             y=proc_df_pedestrian_demand_prob_id_hourly[\"efficiencyRatioAvg\"],\n",
    "#             error_y=dict(\n",
    "#                 type=\"data\",\n",
    "#                 symmetric=False,\n",
    "#                 array=yerr_upper,\n",
    "#                 arrayminus=yerr_lower,\n",
    "#                 color=\"blue\",\n",
    "#                 thickness=1.5,\n",
    "#                 width=3,\n",
    "#             ),\n",
    "#             mode=\"markers+lines\",\n",
    "#             marker=dict(size=8),\n",
    "#             name=f\"Phase {phase_no}\",\n",
    "#         ),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "#     # Add a horizontal threshold line\n",
    "#     fig.add_shape(\n",
    "#         type=\"line\",\n",
    "#         x0=0,\n",
    "#         x1=23,\n",
    "#         y0=1,\n",
    "#         y1=1,\n",
    "#         line=dict(color=\"red\", dash=\"dash\"),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     # title=\"Proportion of Cycles Recommended with PR by Hour with 95% Confidence Intervals\",\n",
    "#     height=800,\n",
    "#     width=1400,\n",
    "#     showlegend=False,\n",
    "#     xaxis_title=\"Hour of Day\",\n",
    "#     yaxis_title=\"Efficiency Ratio\",\n",
    "#     yaxis=dict(range=[0, 1]),  # Set y-axis range\n",
    "#     font=dict(size=14)\n",
    "# )\n",
    "\n",
    "# # Update axis labels for shared x/y axes\n",
    "# fig.update_xaxes(\n",
    "#     title_text=\"Hour of Day\",\n",
    "#     # dtick=1,\n",
    "#     # tickvals=list(range(24)),\n",
    "#     # ticktext=[f\"{hour}:00\" for hour in range(24)],\n",
    "#     # tickformat=\"%H:00\",\n",
    "#     # tickangle=-45,\n",
    "#     # row=2, col=1,  # Shared x-axis title position\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Update y-axes for shared configuration\n",
    "# fig.update_yaxes(\n",
    "#     title_text=\"Efficiency Ratio\", \n",
    "#     range=[0, 0.75],  # Set y-axis range for all subplots\n",
    "#     tickvals=np.linspace(0, 1, 11),  # Tick values from 0 to 1 with step 0.1\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Show plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def hour_with_demand_probability(signal_id: str, with_efficiency: bool):\n",
    "    dict_hour_with_demand_probability_id = {\"signalID\": [], \"approachDir\": [], \"phaseNo\": []}\n",
    "    dict_hour_with_demand_probability_id.update(\n",
    "        {str(hour): [] for hour in range(24)}\n",
    "    )\n",
    "    df_config_id = pd.read_csv(f\"../data/interim/atspm/fdot_d5/signal_config/{signal_id}.csv\")\n",
    "    \n",
    "    phase_nos_config = df_config_id[\"phaseNo\"].unique().tolist()\n",
    "    phase_nos_config = [int(phase_no) for phase_no in phase_nos_config if pd.notna(phase_no) and phase_no % 2 == 0]\n",
    "    \n",
    "    try:\n",
    "        _, df_pedestrian_demand_prob_id_hourly = calculate_ci_with_demand_probability(signal_id=signal_id)\n",
    "        \n",
    "        phase_nos_demand = df_pedestrian_demand_prob_id_hourly[\"phaseNo\"].unique().tolist()\n",
    "        phase_nos_demand = [int(phase_no) for phase_no in phase_nos_demand]\n",
    "    \n",
    "        sequence = [2, 6, 4, 8]\n",
    "        phase_nos = sorted(list(set(phase_nos_config + phase_nos_demand)), key=lambda x: sequence.index(x))\n",
    "        \n",
    "        for phase_no in phase_nos:\n",
    "            dict_hour_with_demand_probability_id[\"signalID\"].append(signal_id)\n",
    "    \n",
    "            if phase_no in phase_nos_config:\n",
    "                approach_dir = df_config_id[df_config_id[\"phaseNo\"] == phase_no][\"approach\"].unique()[0].split()[-1][0:-1]\n",
    "            else:\n",
    "                approach_dir = \"NA\"\n",
    "        \n",
    "            dict_hour_with_demand_probability_id[\"approachDir\"].append(approach_dir)\n",
    "            dict_hour_with_demand_probability_id[\"phaseNo\"].append(phase_no)\n",
    "        \n",
    "            for i in range(24):\n",
    "                proc_df_pedestrian_demand_prob_id_hourly = (\n",
    "                    df_pedestrian_demand_prob_id_hourly\n",
    "                    .query(\"hour == @i & phaseNo == @phase_no\")\n",
    "                )\n",
    "        \n",
    "                if len(proc_df_pedestrian_demand_prob_id_hourly) == 0:\n",
    "                    dict_hour_with_demand_probability_id[str(i)].append(np.nan)\n",
    "                else:\n",
    "                    proc_df_pedestrian_demand_prob_id_hourly = proc_df_pedestrian_demand_prob_id_hourly.reset_index(drop=True)\n",
    "                    demand_prob_lower = proc_df_pedestrian_demand_prob_id_hourly.loc[0, \"demandProbabilityAvgLower\"]\n",
    "                    efficiency_prob_lower = proc_df_pedestrian_demand_prob_id_hourly.loc[0, \"efficiencyRatioAvgLower\"]\n",
    "                    \n",
    "                    if demand_prob_lower >= 0.6:\n",
    "                        dict_hour_with_demand_probability_id[str(i)].append(1)\n",
    "                    elif demand_prob_lower >= 0.4:\n",
    "                        if with_efficiency:\n",
    "                            if efficiency_prob_lower >= 1:\n",
    "                                dict_hour_with_demand_probability_id[str(i)].append(1)\n",
    "                            else:\n",
    "                                dict_hour_with_demand_probability_id[str(i)].append(0)\n",
    "                        else:\n",
    "                            dict_hour_with_demand_probability_id[str(i)].append(1)\n",
    "                    else:\n",
    "                        dict_hour_with_demand_probability_id[str(i)].append(0)\n",
    "    \n",
    "        return pd.DataFrame(dict_hour_with_demand_probability_id)\n",
    "    except:\n",
    "        for phase_no in phase_nos_config:\n",
    "            dict_hour_with_demand_probability_id[\"signalID\"].append(signal_id)\n",
    "            dict_hour_with_demand_probability_id[\"approachDir\"].append(\"NA\")\n",
    "            dict_hour_with_demand_probability_id[\"phaseNo\"].append(np.nan)\n",
    "                                                                      \n",
    "            for i in range(24):\n",
    "                dict_hour_with_demand_probability_id[str(i)].append(np.nan)\n",
    "\n",
    "        return pd.DataFrame(dict_hour_with_demand_probability_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_hour_with_demand_probability = pd.DataFrame()\n",
    "\n",
    "# for signal_id in tqdm.tqdm(signal_ids):\n",
    "#     df_hour_with_demand_probability_id = hour_with_demand_probability(signal_id=signal_id, with_efficiency=False)\n",
    "#     df_hour_with_demand_probability = (\n",
    "#         pd.concat([df_hour_with_demand_probability, df_hour_with_demand_probability_id], axis=0, ignore_index=True)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float_to_int(df_hour_with_demand_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pedestrian Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_ci_with_delay(signal_id):\n",
    "    df_pedestrian_delay_id = load_data(\n",
    "        dirpath=\"../data/production/atspm/fdot_d5/feature_extraction/feature/cycle/pedestrian_traffic/delay\",\n",
    "        signal_id=signal_id\n",
    "    )\n",
    "    columns = [\n",
    "        col for col in df_pedestrian_delay_id.columns if all(suffix not in col for suffix in [\"CurrCycle\", \"PrevCycle\"]) and (\"Delay\" in col) and df_pedestrian_delay_id[col].dtype != \"O\"\n",
    "    ]\n",
    "    \n",
    "    df_pedestrian_delay_id[\"hour\"] = df_pedestrian_delay_id[\"cycleBegin\"].dt.hour\n",
    "    df_pedestrian_delay_id = (\n",
    "        pd.melt(df_pedestrian_delay_id, \n",
    "                id_vars=[\"signalID\", \"date\", \"hour\"], \n",
    "                var_name=\"phaseNo\",\n",
    "                value_vars=columns,\n",
    "                value_name=\"pedestrianDelay\"\n",
    "               )\n",
    "    )\n",
    "    df_pedestrian_delay_id[\"phaseNo\"] = df_pedestrian_delay_id[\"phaseNo\"].str.extract(r'Phase(\\d)').astype(int)\n",
    "\n",
    "    # Filter non-zero values\n",
    "    df_pedestrian_delay_id = (\n",
    "        df_pedestrian_delay_id[df_pedestrian_delay_id[\"pedestrianDelay\"] > 0]\n",
    "    )\n",
    "    # df_pedestrian_delay_id[\"recommendation\"] = (\n",
    "    #     df_pedestrian_delay_id[\"pedestrianDelay\"].apply(lambda x: 1 if x>=60 else 0)\n",
    "    # )\n",
    "    df_pedestrian_delay_id_hourly = (\n",
    "        df_pedestrian_delay_id\n",
    "        .groupby([\"signalID\", \"date\", \"hour\", \"phaseNo\"])\n",
    "        .agg(\n",
    "            pedestrianDelayAvg=(\"pedestrianDelay\", \"mean\")\n",
    "        )\n",
    "        .reset_index()  \n",
    "    )\n",
    "\n",
    "    # Group by `phaseNo` and `hour` and calculate bootstrap confidence intervals and mean\n",
    "    pedestrian_delays_id = []\n",
    "    for (signal_id, hour, phase_no), group in df_pedestrian_delay_id_hourly.groupby([\"signalID\", \"hour\", \"phaseNo\"]):\n",
    "        pedestrian_delays = group[\"pedestrianDelayAvg\"].values\n",
    "        \n",
    "        pedestrian_delay_lower, pedestrian_delay_upper = bootstrap_ci(pedestrian_delays)\n",
    "        pedestrian_delay_mean = np.mean(pedestrian_delays)  # Calculate the mean\n",
    "        \n",
    "        pedestrian_delays_id.append(\n",
    "            {\n",
    "                \"signalID\": signal_id, \"hour\": hour, \"phaseNo\": phase_no, \n",
    "                \"pedestrianDelayAvg\": pedestrian_delay_mean, \n",
    "                \"pedestrianDelayAvgLower\": pedestrian_delay_lower, \n",
    "                \"pedestrianDelayAvgUpper\": pedestrian_delay_upper\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Create a results DataFrame\n",
    "    df_pedestrian_delay_id_hourly = pd.DataFrame(pedestrian_delays_id)\n",
    "\n",
    "    return df_pedestrian_delay_id, df_pedestrian_delay_id_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# _, df_pedestrian_delay_id_hourly = calculate_ci_with_delay(signal_id=\"1500\")\n",
    "# phase_nos = df_pedestrian_delay_id_hourly[\"phaseNo\"].unique()\n",
    "\n",
    "# # Create a 2x2 grid for subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2, cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase_no}\" for phase_no in phase_nos],\n",
    "#     shared_xaxes=False, shared_yaxes=False,\n",
    "#     vertical_spacing=0.15\n",
    "# )\n",
    "\n",
    "# # Iterate over phases and add traces\n",
    "# row_col_mapping = [(1, 1), (1, 2), (2, 1), (2, 2)]  # Map phases to subplot positions\n",
    "# for idx, (phase_no, (row, col)) in enumerate(zip(phase_nos, row_col_mapping)):\n",
    "#     proc_df_pedestrian_delay_id_hourly = df_pedestrian_delay_id_hourly[df_pedestrian_delay_id_hourly[\"phaseNo\"] == phase_no]\n",
    "    \n",
    "#     # Calculate error bars\n",
    "#     yerr_lower = (\n",
    "#         proc_df_pedestrian_delay_id_hourly[\"pedestrianDelayAvg\"] - proc_df_pedestrian_delay_id_hourly[\"pedestrianDelayAvgLower\"]\n",
    "#     )\n",
    "#     yerr_upper = (\n",
    "#         proc_df_pedestrian_delay_id_hourly[\"pedestrianDelayAvgUpper\"] - proc_df_pedestrian_delay_id_hourly[\"pedestrianDelayAvg\"]\n",
    "#     )\n",
    "\n",
    "#     # Add scatter plot with error bars\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=proc_df_pedestrian_delay_id_hourly[\"hour\"],\n",
    "#             y=proc_df_pedestrian_delay_id_hourly[\"pedestrianDelayAvg\"],\n",
    "#             error_y=dict(\n",
    "#                 type=\"data\",\n",
    "#                 symmetric=False,\n",
    "#                 array=yerr_upper,\n",
    "#                 arrayminus=yerr_lower,\n",
    "#                 color=\"blue\",\n",
    "#                 thickness=1.5,\n",
    "#                 width=3,\n",
    "#             ),\n",
    "#             mode=\"markers\",\n",
    "#             marker=dict(size=8),\n",
    "#             name=f\"Phase {phase_no}\",\n",
    "#         ),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "#     # Add a horizontal threshold line\n",
    "#     fig.add_shape(\n",
    "#         type=\"line\",\n",
    "#         x0=0,\n",
    "#         x1=23,\n",
    "#         y0=45,\n",
    "#         y1=45,\n",
    "#         line=dict(color=\"red\", dash=\"dash\"),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     # title=\"Proportion of Cycles Recommended with PR by Hour with 95% Confidence Intervals\",\n",
    "#     height=800,\n",
    "#     width=1400,\n",
    "#     showlegend=False,\n",
    "#     xaxis_title=\"Hour of Day\",\n",
    "#     yaxis_title=\"Pedestrian Delay (sec)\",\n",
    "#     font=dict(size=14)\n",
    "# )\n",
    "\n",
    "# # Update axis labels for shared x/y axes\n",
    "# fig.update_xaxes(\n",
    "#     title_text=\"Hour of Day\",\n",
    "#     # dtick=1,\n",
    "#     # tickvals=list(range(24)),\n",
    "#     # ticktext=[f\"{hour}:00\" for hour in range(24)],\n",
    "#     # tickformat=\"%H:00\",\n",
    "#     # tickangle=-45,\n",
    "#     # row=2, col=1,  # Shared x-axis title position\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Update y-axes for shared configuration\n",
    "# fig.update_yaxes(\n",
    "#     title_text=\"Pedestrian Delay (sec)\", \n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Export the Plotly figure as a high-resolution image\n",
    "# fig.write_image(\"../reports/6.2.png\", width=1400, height=800, scale=2)\n",
    "\n",
    "# # Show plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def hour_with_delay(signal_id):\n",
    "    dict_hour_with_delay_id = {\"signalID\": [], \"approachDir\": [], \"phaseNo\": []}\n",
    "    dict_hour_with_delay_id.update(\n",
    "        {str(hour): [] for hour in range(24)}\n",
    "    )\n",
    "    df_config_id = pd.read_csv(f\"../data/interim/atspm/fdot_d5/signal_config/{signal_id}.csv\")\n",
    "    \n",
    "    phase_nos_config = df_config_id[\"phaseNo\"].unique().tolist()\n",
    "    phase_nos_config = [int(phase_no) for phase_no in phase_nos_config if pd.notna(phase_no) and phase_no % 2 == 0]\n",
    "    \n",
    "    try:\n",
    "        _, df_pedestrian_delay_id_hourly = calculate_ci_with_delay(signal_id=signal_id)\n",
    "        \n",
    "        phase_nos_demand = df_pedestrian_delay_id_hourly[\"phaseNo\"].unique().tolist()\n",
    "        phase_nos_demand = [int(phase_no) for phase_no in phase_nos_demand]\n",
    "    \n",
    "        sequence = [2, 6, 4, 8]\n",
    "        phase_nos = sorted(list(set(phase_nos_config + phase_nos_demand)), key=lambda x: sequence.index(x))\n",
    "        \n",
    "        for phase_no in phase_nos:\n",
    "            dict_hour_with_delay_id[\"signalID\"].append(signal_id)\n",
    "    \n",
    "            if phase_no in phase_nos_config:\n",
    "                approach_dir = df_config_id[df_config_id[\"phaseNo\"] == phase_no][\"approach\"].unique()[0].split()[-1][0:-1]\n",
    "            else:\n",
    "                approach_dir = \"NA\"\n",
    "        \n",
    "            dict_hour_with_delay_id[\"approachDir\"].append(approach_dir)\n",
    "            dict_hour_with_delay_id[\"phaseNo\"].append(phase_no)\n",
    "        \n",
    "            for i in range(24):\n",
    "                proc_df_pedestrian_delay_id_hourly = (\n",
    "                    df_pedestrian_delay_id_hourly\n",
    "                    .query(\"hour == @i & phaseNo == @phase_no\")\n",
    "                )\n",
    "        \n",
    "                if len(proc_df_pedestrian_delay_id_hourly) == 0:\n",
    "                    dict_hour_with_delay_id[str(i)].append(np.nan)\n",
    "                else:\n",
    "                    proc_df_pedestrian_delay_id_hourly = proc_df_pedestrian_delay_id_hourly.reset_index(drop=True)\n",
    "                    pedestrian_delay_lower = proc_df_pedestrian_delay_id_hourly.loc[0, \"pedestrianDelayAvgLower\"]\n",
    "                    \n",
    "                    if pedestrian_delay_lower >= 45: # HCM 2022\n",
    "                        dict_hour_with_delay_id[str(i)].append(1)\n",
    "                    else:\n",
    "                        dict_hour_with_delay_id[str(i)].append(0)\n",
    "    \n",
    "        return pd.DataFrame(dict_hour_with_delay_id)\n",
    "    except:\n",
    "        for phase_no in phase_nos_config:\n",
    "            dict_hour_with_delay_id[\"signalID\"].append(signal_id)\n",
    "            dict_hour_with_delay_id[\"approachDir\"].append(\"NA\")\n",
    "            dict_hour_with_delay_id[\"phaseNo\"].append(np.nan)\n",
    "                                                                      \n",
    "            for i in range(24):\n",
    "                dict_hour_with_delay_id[str(i)].append(np.nan)\n",
    "\n",
    "        return pd.DataFrame(dict_hour_with_delay_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hour_with_delay = pd.DataFrame()\n",
    "\n",
    "# for signal_id in tqdm.tqdm(signal_ids):\n",
    "#     df_critical_hour_with_delat_id = hour_with_delay(signal_id=signal_id)\n",
    "#     df_hour_with_delay = (\n",
    "#         pd.concat([df_hour_with_delay, df_critical_hour_with_delay_id], axis=0, ignore_index=True)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float_to_int(df_hour_with_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pr_recommendations = (\n",
    "#     float_to_int(df_hour_with_demand_probability.iloc[:, 3:].replace(np.nan, 1)).values \n",
    "#     * \n",
    "#     float_to_int(df_hour_with_delay.iloc[:, 3:].replace(np.nan, 1)).values\n",
    "# )\n",
    "# data = np.hstack(\n",
    "#     [df_hour_with_demand_probability[[\"signalID\", \"approachDir\", \"phaseNo\"]].values, pr_recommendations]\n",
    "# )\n",
    "\n",
    "# df_pr_recommendations = pd.DataFrame(data, columns=df_hour_with_demand_probability.columns)\n",
    "# df_pr_recommendations[\"phaseNo\"] = df_pr_recommendations[[\"phaseNo\"]].astype(\"Int64\")\n",
    "\n",
    "# df_pr_recommendations.to_csv(\"../reports/recommendations/pr_recommendation_with_efficiency.csv\", \n",
    "#                              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pr_recommendations = (\n",
    "#     float_to_int(df_hour_with_demand_probability.iloc[:, 3:].replace(np.nan, 1)).values \n",
    "#     * \n",
    "#     float_to_int(df_hour_with_delay.iloc[:, 3:].replace(np.nan, 1)).values\n",
    "# )\n",
    "# data = np.hstack(\n",
    "#     [df_hour_with_demand_probability[[\"signalID\", \"approachDir\", \"phaseNo\"]].values, pr_recommendations]\n",
    "# )\n",
    "\n",
    "# df_pr_recommendations = pd.DataFrame(data, columns=df_hour_with_demand_probability.columns)\n",
    "# df_pr_recommendations[\"phaseNo\"] = df_pr_recommendations[[\"phaseNo\"]].astype(\"Int64\")\n",
    "\n",
    "# df_pr_recommendations.to_csv(\"../reports/recommendations/pr_recommendation_without_efficiency.csv\", \n",
    "#                              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# proc_df_pr_recommendations = df_pr_recommendations[df_pr_recommendations[\"signalID\"] == \"1500\"]\n",
    "\n",
    "# # Create a 2x2 grid for the subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2,\n",
    "#     cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase}\" for phase in proc_df_pr_recommendations[\"phaseNo\"].unique()],\n",
    "#     shared_xaxes=False,\n",
    "#     # shared_yaxes=False,\n",
    "#     vertical_spacing=0.6\n",
    "# )\n",
    "\n",
    "# # Define the color mapping for PR recommendations\n",
    "# colors = {0: \"#27ae60\", 1: \"#cb4335\"}  # Green: PR Not Needed, Red: PR Needed\n",
    "\n",
    "# # Add a heatmap for each phase in the 2x2 layout\n",
    "# phase_nos = proc_df_pr_recommendations[\"phaseNo\"].unique()\n",
    "\n",
    "# for idx, phase_no in enumerate(phase_nos, start=1):\n",
    "#     # Get subplot row and column\n",
    "#     row, col = divmod(idx - 1, 2)\n",
    "#     row += 1\n",
    "#     col += 1\n",
    "\n",
    "#     # Filter data for the current phase\n",
    "#     proc_df_pr_recommendations_phase = (\n",
    "#         proc_df_pr_recommendations[proc_df_pr_recommendations[\"phaseNo\"] == phase_no].iloc[:, 3:]\n",
    "#     )  # Extract hourly binary data\n",
    "    \n",
    "#     pr_recommendations_phase = proc_df_pr_recommendations_phase.values  # Convert to matrix\n",
    "#     x_labels = list(range(24))  # Hours\n",
    "#     y_labels = [\n",
    "#         f\"{row['signalID']} ({row['approachDir']})\" for _, row in proc_df_pr_recommendations[proc_df_pr_recommendations[\"phaseNo\"] == phase_no].iterrows()\n",
    "#     ]\n",
    "\n",
    "#     # Add heatmap trace\n",
    "#     fig.add_trace(\n",
    "#         go.Heatmap(\n",
    "#             z=pr_recommendations_phase,\n",
    "#             x=x_labels,\n",
    "#             y=y_labels,\n",
    "#             colorscale=[[0, \"#27ae60\"], [1, \"#cb4335\"]],  # Define two discrete colors\n",
    "#             showscale=False,  # Disable color bar\n",
    "#             zmin=0,\n",
    "#             zmax=0.5,\n",
    "#         ),\n",
    "#         row=row,\n",
    "#         col=col,\n",
    "#     )\n",
    "\n",
    "# # Add a legend manually\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(size=14, color=\"#27ae60\"),\n",
    "#         name=\"PR Not Recommended\",\n",
    "#     )\n",
    "# )\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(size=14, color=\"#cb4335\"),\n",
    "#         name=\"PR Recommended\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     height=400,\n",
    "#     width=1400,\n",
    "#     # title=\"Hourly Signal PR Recommendations for Each Phase\",\n",
    "#     # yaxis=dict(title=\"Signal ID (Direction)\"),\n",
    "# )\n",
    "\n",
    "# # Update layout with legend font size\n",
    "# fig.update_layout(\n",
    "#     legend=dict(\n",
    "#         orientation=\"h\",  # Horizontal legend\n",
    "#         x=0.5,            # Center horizontally\n",
    "#         y=-0.4,           # Position below the plot\n",
    "#         xanchor=\"center\",\n",
    "#         yanchor=\"top\",\n",
    "#         font=dict(size=14),  # Set font size for the legend items\n",
    "#         # title=dict(text=\"Status\", font=dict(size=14)),  # Title with font size\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Customize x-axis for each subplot\n",
    "# for i in range(1, len(phase_nos) + 1):\n",
    "#     fig.update_xaxes(\n",
    "#         title=\"Hour of Day\",\n",
    "#         tickmode=\"array\",\n",
    "#         tickvals=list(range(24)),\n",
    "#         ticktext=[f\"{hour:02d}:00\" for hour in range(24)],\n",
    "#         row=((i - 1) // 2) + 1,\n",
    "#         col=((i - 1) % 2) + 1,\n",
    "#         tickangle=-90,\n",
    "#         title_font=dict(size=14),\n",
    "#         tickfont=dict(size=14),\n",
    "#     )\n",
    "\n",
    "#     fig.update_yaxes(\n",
    "#         title_font=dict(size=16),\n",
    "#         tickfont=dict(size=14),\n",
    "#     )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# proc_df_recommendation_with_delay = (\n",
    "#     df_recommendation_with_delay\n",
    "#     .groupby([\"signalID\"])\n",
    "#     .agg(\n",
    "#         **{\n",
    "#             str(col): (str(col), \"max\")  \n",
    "#             for col in range(24)            \n",
    "#         }\n",
    "#     )\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "# proc_df_hour_with_demand_probability = (\n",
    "#     df_hour_with_demand_probability\n",
    "#     .groupby([\"signalID\"])\n",
    "#     .agg(\n",
    "#         **{\n",
    "#             str(col): (str(col), \"max\")  \n",
    "#             for col in range(24)            \n",
    "#         }\n",
    "#     )\n",
    "#     .reset_index()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# recommendations = (\n",
    "#     proc_df_recommendation_with_delay.iloc[:, 1:].values * proc_df_hour_with_demand_probability.iloc[:, 1:].values\n",
    "# )\n",
    "# data = np.hstack([np.array(signal_ids).reshape(-1, 1), recommendations])\n",
    "\n",
    "# df_pr_recommendations = pd.DataFrame(data, columns=proc_df_recommendation_with_delay.columns)\n",
    "# df_pr_recommendations.to_csv(\"../reports/recommendations/intersection_level/pr_recommendation_with_efficiency.csv\", \n",
    "#                              index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation of Pedestrian Presence Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_hourly_activity_summary(signal_id: str):\n",
    "    df_spat_id = (\n",
    "        load_data(\n",
    "            dirpath=\"../data/production/atspm/fdot_d5/feature_extraction/feature/cycle/vehicle_signal/spat/\",\n",
    "            signal_id=signal_id\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_pedestrian_activity_id = (\n",
    "        load_data(\n",
    "            dirpath=\"../data/production/atspm/fdot_d5/feature_extraction/feature/cycle/pedestrian_traffic/activity\",\n",
    "            signal_id=signal_id\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    columns = [\n",
    "        column for column in df_pedestrian_activity_id.columns if not any(k in column for k in [\"45\"])\n",
    "    ]\n",
    "    df_pedestrian_activity_id = df_pedestrian_activity_id[columns]\n",
    "    df_pedestrian_activity_id = float_to_int(df_pedestrian_activity_id)\n",
    "    \n",
    "    # Join\n",
    "    df_pedestrian_activity_id = pd.merge(\n",
    "        df_spat_id[[\"signalID\", \"date\", \"cycleNo\", \"cycleBegin\", \"cycleEnd\"]], df_pedestrian_activity_id, \n",
    "        on=[\"signalID\", \"date\", \"cycleNo\", \"cycleBegin\", \"cycleEnd\"], \n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Shift\n",
    "    columns = [column for column in df_pedestrian_activity_id.columns if \"Prev\" in column]\n",
    "    \n",
    "    for column in columns:\n",
    "        df_pedestrian_activity_id[column] = df_pedestrian_activity_id[column].shift(-1)\n",
    "    \n",
    "    df_pedestrian_activity_id = df_pedestrian_activity_id.fillna(0)\n",
    "    \n",
    "    phase_nos = [\n",
    "        int(column[-1]) for column in df_pedestrian_activity_id.columns if \"90\" in column and not any(k in column for k in [\"Cycle\"])\n",
    "    ]\n",
    "    \n",
    "    dict_indicator = {\n",
    "        f\"pedestrianActivityIndicatorPhase{phase_no}\": [] for phase_no in phase_nos\n",
    "    }\n",
    "    \n",
    "    for i in range(len(df_pedestrian_activity_id)):\n",
    "        for phase_no in phase_nos:\n",
    "            cycle_no_curr = df_pedestrian_activity_id.loc[i, \"cycleNo\"]\n",
    "            pedestrian_activity_curr = df_pedestrian_activity_id.loc[i, f\"pedestrianActivity90Phase{phase_no}CurrCycle\"]\n",
    "            pedestrian_activity_prev = df_pedestrian_activity_id.loc[i, f\"pedestrianActivity90Phase{phase_no}PrevCycle\"]\n",
    "            \n",
    "            if i == len(df_pedestrian_activity_id) - 1:\n",
    "                if pedestrian_activity_curr > 0 or pedestrian_activity_prev > 0:\n",
    "                    dict_indicator[f\"pedestrianActivityIndicatorPhase{phase_no}\"].append(1)\n",
    "                else:\n",
    "                    dict_indicator[f\"pedestrianActivityIndicatorPhase{phase_no}\"].append(0)\n",
    "            else:\n",
    "                cycle_no_next = df_pedestrian_activity_id.loc[i+1, \"cycleNo\"]\t\n",
    "                if (cycle_no_next - cycle_no_curr) > 1:\n",
    "                    dict_indicator[f\"pedestrianActivityIndicatorPhase{phase_no}\"].append(0)\n",
    "                else:\n",
    "                    if pedestrian_activity_curr > 0 or pedestrian_activity_prev > 0:\n",
    "                        dict_indicator[f\"pedestrianActivityIndicatorPhase{phase_no}\"].append(1)\n",
    "                    else:\n",
    "                        dict_indicator[f\"pedestrianActivityIndicatorPhase{phase_no}\"].append(0)\n",
    "    \n",
    "    \n",
    "    columns = [column for column in df_pedestrian_activity_id.columns if \"Activity\" in column]\n",
    "    \n",
    "    df_pedestrian_activity_id = df_pedestrian_activity_id.drop(columns=columns)\n",
    "    df_pedestrian_activity_id = pd.concat([df_pedestrian_activity_id, pd.DataFrame(dict_indicator)], \n",
    "                                          axis=1)\n",
    "    \n",
    "    df_pedestrian_activity_id[\"hour\"] = df_pedestrian_activity_id[\"cycleBegin\"].dt.hour\n",
    "        \n",
    "    activity_columns = [col for col in df_pedestrian_activity_id.columns if \"Activity\" in col]\n",
    "    \n",
    "    df_pedestrian_activity_id_hourly = (\n",
    "        df_pedestrian_activity_id\n",
    "        .groupby([\"signalID\", \"date\", \"hour\"])[activity_columns]\n",
    "        .agg(['sum', 'count'])\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    df_pedestrian_activity_id_hourly.columns = [\n",
    "        ''.join([col[0], col[1].capitalize()]).rstrip('_') if isinstance(col, tuple) else col\n",
    "        for col in df_pedestrian_activity_id_hourly.columns\n",
    "    ] # Sum: Count of 1s; Count: Count of 1s and 0s\n",
    "\n",
    "    sum_columns = [col for col in df_pedestrian_activity_id_hourly.columns if 'Sum' in col]\n",
    "    df_pedestrian_activity_id_hourly_sum = pd.melt(\n",
    "        df_pedestrian_activity_id_hourly, \n",
    "        id_vars=[\"signalID\", \"date\", \"hour\"], \n",
    "        value_vars=sum_columns, \n",
    "        var_name=\"phaseNo\", \n",
    "        value_name=\"cyclesWithPedestrian\"\n",
    "    )\n",
    "    df_pedestrian_activity_id_hourly_sum[\"phaseNo\"] = (\n",
    "        df_pedestrian_activity_id_hourly_sum[\"phaseNo\"].str.extract(r'Phase(\\d+)').astype(int)\n",
    "    )\n",
    "\n",
    "    count_columns = [col for col in df_pedestrian_activity_id_hourly.columns if 'Count' in col]\n",
    "    df_pedestrian_activity_id_hourly_count = pd.melt(\n",
    "        df_pedestrian_activity_id_hourly, \n",
    "        id_vars=[\"signalID\", \"date\", \"hour\"], \n",
    "        value_vars=count_columns, \n",
    "        var_name=\"phaseNo\", \n",
    "        value_name=\"totalCycles\"\n",
    "    )\n",
    "    df_pedestrian_activity_id_hourly_count[\"phaseNo\"] = (\n",
    "        df_pedestrian_activity_id_hourly_count[\"phaseNo\"].str.extract(r'Phase(\\d+)').astype(int)\n",
    "    )\n",
    "    \n",
    "    df_pedestrian_activity_id_hourly = pd.merge(\n",
    "        df_pedestrian_activity_id_hourly_sum, df_pedestrian_activity_id_hourly_count, \n",
    "        on=[\"signalID\", \"date\", \"hour\", \"phaseNo\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    return df_pedestrian_activity_id_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_pedestrian_activity_id_hourly = calculate_hourly_activity_summary(signal_id=\"1500\")\n",
    "\n",
    "# # Get unique phases\n",
    "# phase_nos = sorted(df_pedestrian_activity_id_hourly[\"phaseNo\"].unique())\n",
    "\n",
    "# # Create a 2x2 grid of subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2,\n",
    "#     cols=2,\n",
    "#     subplot_titles=[f\"Phase No: {phase_no}\" for phase_no in phase_nos],\n",
    "#     shared_yaxes=False,  \n",
    "#     vertical_spacing=0.15,\n",
    "# )\n",
    "\n",
    "# # Map phase to subplot positions\n",
    "# row_col_mapping = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "\n",
    "# # Add boxplots for each phase\n",
    "# for phase_no, (row, col) in zip(phase_nos, row_col_mapping):\n",
    "#     df_pedestrian_activity_id_hourly_phase = (\n",
    "#         df_pedestrian_activity_id_hourly[df_pedestrian_activity_id_hourly[\"phaseNo\"] == phase_no]\n",
    "#     )\n",
    "#     df_pedestrian_activity_id_hourly_phase = df_pedestrian_activity_id_hourly_phase.copy()\n",
    "\n",
    "#     df_pedestrian_activity_id_hourly_phase[\"pedestrianProportion\"] = (\n",
    "#         df_pedestrian_activity_id_hourly_phase['cyclesWithPedestrian'] / df_pedestrian_activity_id_hourly_phase['totalCycles']\n",
    "#     )\n",
    "\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=df_pedestrian_activity_id_hourly_phase[\"hour\"],\n",
    "#             y=df_pedestrian_activity_id_hourly_phase[\"pedestrianProportion\"],\n",
    "#             # boxpoints='all', \n",
    "#             # jitter=0.3,  \n",
    "#             # pointpos=-1.8,  \n",
    "#             name=f\"Phase No: {phase_no}\",  # Legend name (optional)\n",
    "#             marker_color=\"#408eb3\",\n",
    "#         ),\n",
    "#         row=row,\n",
    "#         col=col\n",
    "#     )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     height=800,\n",
    "#     width=1400,\n",
    "#     title_text=\"Day-to-Day Variability in Pedestrian Presence by Hour\",\n",
    "#     showlegend=False,  # Optional: Hide legend as subplot titles already explain phases\n",
    "#     font=dict(size=14),\n",
    "# )\n",
    "\n",
    "# # Set consistent Y-axis range for all subplots (0 to 1 for proportions)\n",
    "# fig.update_yaxes(\n",
    "#     # range=[0, 1], \n",
    "#     title_text='Proportion of Cycles with Pedestrians'\n",
    "# )\n",
    "# fig.update_xaxes(title_text='Hour of Day')\n",
    "\n",
    "# # Export the figure as a high-resolution image\n",
    "# fig.write_image(\"pedestrian_proportion.png\",  width=1400, height=800, scale=2)\n",
    "\n",
    "# # Display plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def beta_binomial_log_likelihood(params, successes, trials):\n",
    "    alpha, beta = params\n",
    "    # Log-likelihood for the Beta-Binomial model\n",
    "    log_likelihood = np.sum(\n",
    "        betaln(successes + alpha, trials - successes + beta) - betaln(alpha, beta)\n",
    "    )\n",
    "    return -log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_ci_with_pedestrian_presence_probability(signal_id: str):\n",
    "    df_pedestrian_activity_id_hourly = calculate_hourly_activity_summary(signal_id=signal_id)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Get unique phases\n",
    "    phase_nos = sorted(df_pedestrian_activity_id_hourly[\"phaseNo\"].unique())\n",
    "    \n",
    "    for phase_no in phase_nos:\n",
    "        df_pedestrian_activity_id_hourly_phase = (\n",
    "            df_pedestrian_activity_id_hourly[df_pedestrian_activity_id_hourly[\"phaseNo\"] == phase_no]\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        signal_id = df_pedestrian_activity_id_hourly_phase.loc[0, \"signalID\"]\n",
    "        \n",
    "        for hour in np.arange(0, 24):\n",
    "            df_pedestrian_activity_id_hourly_hour = (\n",
    "                df_pedestrian_activity_id_hourly_phase[df_pedestrian_activity_id_hourly_phase[\"hour\"] == hour]\n",
    "            )\n",
    "            successes = df_pedestrian_activity_id_hourly_hour[\"cyclesWithPedestrian\"].values\n",
    "            trials = df_pedestrian_activity_id_hourly_hour[\"totalCycles\"].values\n",
    "        \n",
    "            # Skip hour if no data\n",
    "            if len(successes) == 0:\n",
    "                continue\n",
    "        \n",
    "            result = minimize(\n",
    "                beta_binomial_log_likelihood,\n",
    "                x0=[1, 1],\n",
    "                args=(successes, trials),\n",
    "                bounds=[(0.01, None), (0.01, None)],\n",
    "            )\n",
    "        \n",
    "            alpha_hat, beta_hat = result.x\n",
    "            mean_prob = alpha_hat / (alpha_hat + beta_hat)\n",
    "        \n",
    "            # 95% credible interval using Beta distribution\n",
    "            lower_bound = beta.ppf(0.025, alpha_hat, beta_hat)\n",
    "            upper_bound = beta.ppf(0.975, alpha_hat, beta_hat)\n",
    "        \n",
    "            results.append({\n",
    "                'signalID': signal_id,\n",
    "                'phaseNo': phase_no,\n",
    "                'hour': hour,\n",
    "                'alpha': alpha_hat,\n",
    "                'beta': beta_hat,\n",
    "                'probability': mean_prob,\n",
    "                'lowerBound': lower_bound,\n",
    "                'upperBound': upper_bound\n",
    "            })\n",
    "    \n",
    "    df_pedestrian_presence_probability_id_hourly = pd.DataFrame(results)\n",
    "\n",
    "    return df_pedestrian_presence_probability_id_hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identification of Critical Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df():\n",
    "    df_pedestrian_presence_probability_hourly = pd.DataFrame()\n",
    "    \n",
    "    for signal_id in tqdm.tqdm(signal_ids):\n",
    "        try:\n",
    "            df_pedestrian_presence_probability_id_hourly = calculate_ci_with_pedestrian_presence_probability(signal_id=signal_id)\n",
    "            df_pedestrian_presence_probability_hourly = pd.concat(\n",
    "                [df_pedestrian_presence_probability_hourly, df_pedestrian_presence_probability_id_hourly], \n",
    "                axis=0, ignore_index=True\n",
    "            )\n",
    "        except:\n",
    "            print(signal_id)\n",
    "    \n",
    "    df_pedestrian_presence_probability_hourly = (\n",
    "        df_pedestrian_presence_probability_hourly[df_pedestrian_presence_probability_hourly[\"phaseNo\"].isin([2, 4, 6, 8])]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df_pedestrian_presence_probability_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Filter and reshape data\n",
    "# # turn_conflict_propensity_scores = (\n",
    "# #     df_turn_conflict_propensity_scores_id_hourly\n",
    "# #     .loc[:, \"turnConflictPropensity\"]\n",
    "# #     .values\n",
    "# # )\n",
    "\n",
    "# pedestrian_presence_probability_hourly = (\n",
    "#     df_pedestrian_presence_probability_hourly\n",
    "#     .loc[:, \"probability\"]\n",
    "#     .values\n",
    "# )\n",
    "\n",
    "# # Reshape data for K-Means\n",
    "# pedestrian_presence_probability_hourly = pedestrian_presence_probability_hourly.reshape(-1, 1)\n",
    "\n",
    "# # Apply k-means clustering with 2 clusters\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(pedestrian_presence_probability_hourly)\n",
    "\n",
    "# # Get cluster centroids and labels\n",
    "# centroids = kmeans.cluster_centers_.flatten()\n",
    "# labels = kmeans.labels_\n",
    "\n",
    "# # Assuming pedestrian_presence_probability_hourly and labels are of equal length\n",
    "# pedestrian_presence_probability_hourly = pedestrian_presence_probability_hourly.flatten()  # Ensure 1D array\n",
    "# labels = labels.flatten()  # Ensure 1D array\n",
    "\n",
    "# # Sort centroids to define thresholds logically\n",
    "# centroids = np.sort(centroids)\n",
    "# centroid = centroids[1]\n",
    "\n",
    "# # Display thresholds\n",
    "# print(\"Boundaries for Pedestrian Presence Probability:\")\n",
    "# print(f\"Centroid: {centroid}\")\n",
    "\n",
    "# # Create a scatter plot using Plotly\n",
    "# fig = px.scatter(\n",
    "#     x=pedestrian_presence_probability_hourly, \n",
    "#     y=labels.astype(int),  # Ensure labels are integers\n",
    "#     color=labels.astype(str),  # Convert labels to string for color grouping\n",
    "#     labels={\"x\": \"Pedestrian Presence Probability\", \"y\": \"Cluster Label\", \"color\": \"Cluster\"},\n",
    "# )\n",
    "\n",
    "# # Add threshold lines\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=[centroid, centroid],\n",
    "#     y=[-0.5, max(labels) + 0.5],\n",
    "#     mode=\"lines\",\n",
    "#     line=dict(color=\"red\", dash=\"dash\"),\n",
    "#     name=f\"Centroid: {np.round(centroid, 2)}\"\n",
    "# ))\n",
    "\n",
    "# # Update layout with integer y-ticks\n",
    "# fig.update_layout(\n",
    "#     height=600,\n",
    "#     width=1400,\n",
    "#     xaxis=dict(title=dict(text=\"Pedestrian Presence Probability\", font=dict(size=16))),\n",
    "#     yaxis=dict(\n",
    "#         title=dict(text=\"Cluster Label\", font=dict(size=16)),\n",
    "#         tickmode=\"linear\",  # Ensures consistent tick intervals\n",
    "#         tick0=0,  # Starting tick\n",
    "#         dtick=1,  # Interval between ticks to force integers\n",
    "#     ),\n",
    "#     legend=dict(\n",
    "#         title=dict(text=\"Cluster Legend\", font=dict(size=16, color=\"black\")),  # Bold legend title\n",
    "#         font=dict(size=16, color=\"black\"),  # Bold legend items\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Export the Plotly figure as a high-resolution image\n",
    "# fig.write_image(\"../reports/6.1(a).png\", width=1400, height=600, scale=2)\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid():\n",
    "    df_pedestrian_presence_probability_hourly = concat_df()\n",
    "\n",
    "    pedestrian_presence_probability_hourly = (\n",
    "        df_pedestrian_presence_probability_hourly\n",
    "        .loc[:, \"probability\"]\n",
    "        .values\n",
    "    )\n",
    "    \n",
    "    # Reshape data for K-Means\n",
    "    pedestrian_presence_probability_hourly = pedestrian_presence_probability_hourly.reshape(-1, 1)\n",
    "    \n",
    "    # Apply k-means clustering with 2 clusters\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    kmeans.fit(pedestrian_presence_probability_hourly)\n",
    "    \n",
    "    # Get cluster centroids and labels\n",
    "    centroids = kmeans.cluster_centers_.flatten()\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Assuming pedestrian_presence_probability_hourly and labels are of equal length\n",
    "    pedestrian_presence_probability_hourly = pedestrian_presence_probability_hourly.flatten()  # Ensure 1D array\n",
    "    labels = labels.flatten()  # Ensure 1D array\n",
    "    \n",
    "    # Sort centroids to define thresholds logically\n",
    "    centroids = np.sort(centroids)\n",
    "    centroid = centroids[1]\n",
    "\n",
    "    return centroid, df_pedestrian_presence_probability_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid, df_pedestrian_presence_probability_hourly = centroid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_pedestrian_presence_probability_id_hourly = (\n",
    "#     df_pedestrian_presence_probability_hourly[df_pedestrian_presence_probability_hourly[\"signalID\"] == \"1500\"]\n",
    "# )\n",
    "# phase_nos = df_pedestrian_presence_probability_id_hourly[\"phaseNo\"].unique()\n",
    "# phase_nos = [2, 6, 4, 8]\n",
    "\n",
    "# # Create a 2x2 grid for subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2, cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase_no}\" for phase_no in phase_nos],\n",
    "#     shared_xaxes=False, shared_yaxes=False,\n",
    "#     vertical_spacing=0.15\n",
    "# )\n",
    "\n",
    "# # Iterate over phases and add traces\n",
    "# row_col_mapping = [(1, 1), (1, 2), (2, 1), (2, 2)]  # Map phases to subplot positions\n",
    "# for idx, (phase_no, (row, col)) in enumerate(zip(phase_nos, row_col_mapping)):\n",
    "#     proc_df_pedestrian_presence_probability_id_hourly = (\n",
    "#         df_pedestrian_presence_probability_id_hourly[df_pedestrian_presence_probability_id_hourly[\"phaseNo\"] == phase_no]\n",
    "#     )\n",
    "    \n",
    "#     # Calculate error bars\n",
    "#     yerr_lower = (\n",
    "#         proc_df_pedestrian_presence_probability_id_hourly[\"probability\"] - proc_df_pedestrian_presence_probability_id_hourly[\"lowerBound\"]\n",
    "#     )\n",
    "#     yerr_upper = (\n",
    "#         proc_df_pedestrian_presence_probability_id_hourly[\"upperBound\"] - proc_df_pedestrian_presence_probability_id_hourly[\"probability\"]\n",
    "#     )\n",
    "\n",
    "#     # Add scatter plot with error bars\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=proc_df_pedestrian_presence_probability_id_hourly[\"hour\"],\n",
    "#             y=proc_df_pedestrian_presence_probability_id_hourly[\"probability\"],\n",
    "#             error_y=dict(\n",
    "#                 type=\"data\",\n",
    "#                 symmetric=False,\n",
    "#                 array=yerr_upper,\n",
    "#                 arrayminus=yerr_lower,\n",
    "#                 color=\"blue\",\n",
    "#                 thickness=1.5,\n",
    "#                 width=3,\n",
    "#             ),\n",
    "#             mode=\"markers+lines\",\n",
    "#             marker=dict(size=8),\n",
    "#             name=f\"Phase {phase_no}\",\n",
    "#         ),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "#     # Add a horizontal threshold line\n",
    "#     fig.add_shape(\n",
    "#         type=\"line\",\n",
    "#         x0=0,\n",
    "#         x1=23,\n",
    "#         y0=0.13,\n",
    "#         y1=0.13,\n",
    "#         line=dict(color=\"red\", dash=\"dash\"),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "#     # # Add a horizontal threshold line\n",
    "#     # fig.add_shape(\n",
    "#     #     type=\"line\",\n",
    "#     #     x0=0,\n",
    "#     #     x1=23,\n",
    "#     #     y0=0.05,\n",
    "#     #     y1=0.05,\n",
    "#     #     line=dict(color=\"orange\", dash=\"dash\"),\n",
    "#     #     row=row, col=col\n",
    "#     # )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     # title=\"Probability of Pedestrian Presence by Hour\",\n",
    "#     height=800,\n",
    "#     width=1400,\n",
    "#     showlegend=False,\n",
    "#     xaxis_title=\"Hour of Day\",\n",
    "#     yaxis_title=\"Probability of Pedestrian Demand\",\n",
    "#     # yaxis=dict(range=[0, 1]),  \n",
    "#     font=dict(size=14)\n",
    "# )\n",
    "\n",
    "# # Update axis labels for shared x/y axes\n",
    "# fig.update_xaxes(\n",
    "#     title_text=\"Hour of Day\",\n",
    "#     # dtick=1,\n",
    "#     # tickvals=list(range(24)),\n",
    "#     # ticktext=[f\"{hour}:00\" for hour in range(24)],\n",
    "#     # tickformat=\"%H:00\",\n",
    "#     # tickangle=-45,\n",
    "#     # row=2, col=1,  # Shared x-axis title position\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Update y-axes for shared configuration\n",
    "# fig.update_yaxes(\n",
    "#     title_text=\"Probability of Pedestrian Presence\", \n",
    "#     # range=[0, 0.75],  \n",
    "#     tickvals=np.linspace(0, 1, 11),  # Tick values from 0 to 1 with step 0.1\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Export the figure as a high-resolution image\n",
    "# fig.write_image(\"../reports/6.1(b).png\",  width=1400, height=800, scale=2)\n",
    "\n",
    "# # Show plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critical_hour_with_pedestrian_presence_probability(signal_id, centroid):\n",
    "    dict_critical_hour_with_pedestrian_presence_probability_id = {\"signalID\": [], \"approachDir\": [], \"phaseNo\": []}\n",
    "    dict_critical_hour_with_pedestrian_presence_probability_id.update(\n",
    "        {str(hour): [] for hour in range(24)}\n",
    "    )\n",
    "    df_config_id = pd.read_csv(f\"../data/interim/atspm/fdot_d5/signal_config/{signal_id}.csv\")\n",
    "    \n",
    "    phase_nos_config = df_config_id[\"phaseNo\"].unique().tolist()\n",
    "    phase_nos_config = [int(phase_no) for phase_no in phase_nos_config if pd.notna(phase_no) and phase_no % 2 == 0]\n",
    "    \n",
    "    try:\n",
    "        df_pedestrian_presence_probability_id_hourly = calculate_ci_with_pedestrian_presence_probability(signal_id=signal_id)\n",
    "        df_pedestrian_presence_probability_id_hourly = (\n",
    "            df_pedestrian_presence_probability_id_hourly[df_pedestrian_presence_probability_id_hourly[\"phaseNo\"].isin([2, 4, 6, 8])]\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        \n",
    "        phase_nos_demand = df_pedestrian_presence_probability_id_hourly[\"phaseNo\"].unique().tolist()\n",
    "        phase_nos_demand = [int(phase_no) for phase_no in phase_nos_demand]\n",
    "    \n",
    "        sequence = [2, 6, 4, 8]\n",
    "        phase_nos = sorted(list(set(phase_nos_config + phase_nos_demand)), key=lambda x: sequence.index(x))\n",
    "        \n",
    "        for phase_no in phase_nos:\n",
    "            dict_critical_hour_with_pedestrian_presence_probability_id[\"signalID\"].append(signal_id)\n",
    "    \n",
    "            if phase_no in phase_nos_config:\n",
    "                approach_dir = df_config_id[df_config_id[\"phaseNo\"] == phase_no][\"approach\"].unique()[0].split()[-1][0:-1]\n",
    "            else:\n",
    "                approach_dir = \"NA\"\n",
    "        \n",
    "            dict_critical_hour_with_pedestrian_presence_probability_id[\"approachDir\"].append(approach_dir)\n",
    "            dict_critical_hour_with_pedestrian_presence_probability_id[\"phaseNo\"].append(phase_no)\n",
    "        \n",
    "            for i in range(24):\n",
    "                proc_df_pedestrian_presence_probability_id_hourly = (\n",
    "                    df_pedestrian_presence_probability_id_hourly\n",
    "                    .query(\"hour == @i & phaseNo == @phase_no\")\n",
    "                )\n",
    "        \n",
    "                if len(proc_df_pedestrian_presence_probability_id_hourly) == 0:\n",
    "                    dict_critical_hour_with_pedestrian_presence_probability_id[str(i)].append(np.nan)\n",
    "                else:\n",
    "                    proc_df_pedestrian_presence_probability_id_hourly = (\n",
    "                        proc_df_pedestrian_presence_probability_id_hourly\n",
    "                        .reset_index(drop=True)\n",
    "                    )\n",
    "                    pedestrian_presence_probability_lower = (\n",
    "                        proc_df_pedestrian_presence_probability_id_hourly.loc[0, \"lowerBound\"]\n",
    "                    )\n",
    "                    \n",
    "                    if pedestrian_presence_probability_lower >= centroid:\n",
    "                        dict_critical_hour_with_pedestrian_presence_probability_id[str(i)].append(1)\n",
    "                    else:\n",
    "                        dict_critical_hour_with_pedestrian_presence_probability_id[str(i)].append(0)\n",
    "    \n",
    "        return pd.DataFrame(dict_critical_hour_with_pedestrian_presence_probability_id)\n",
    "    except:\n",
    "        for phase_no in phase_nos_config:\n",
    "            dict_critical_hour_with_pedestrian_presence_probability_id[\"signalID\"].append(signal_id)\n",
    "            dict_critical_hour_with_pedestrian_presence_probability_id[\"approachDir\"].append(\"NA\")\n",
    "            dict_critical_hour_with_pedestrian_presence_probability_id[\"phaseNo\"].append(np.nan)\n",
    "                                                                      \n",
    "            for i in range(24):\n",
    "                dict_critical_hour_with_pedestrian_presence_probability_id[str(i)].append(np.nan)\n",
    "\n",
    "        return pd.DataFrame(dict_critical_hour_with_pedestrian_presence_probability_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_critical_hour_with_pedestrian_presence_probability = pd.DataFrame()\n",
    "\n",
    "# for signal_id in tqdm.tqdm(signal_ids):\n",
    "#     df_critical_hour_with_pedestrian_presence_probability_id = critical_hour_with_pedestrian_presence_probability(\n",
    "#         signal_id=signal_id, centroid=centroid\n",
    "#     )\n",
    "#     df_critical_hour_with_pedestrian_presence_probability = (\n",
    "#         pd.concat(\n",
    "#             [df_critical_hour_with_pedestrian_presence_probability, df_critical_hour_with_pedestrian_presence_probability_id], \n",
    "#             axis=0, ignore_index=True)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pr_recommendations = float_to_int(df_critical_hour_with_pedestrian_presence_probability)\n",
    "# df_pr_recommendations.to_csv(\"../reports/recommendations/pr_recommendations.csv\", \n",
    "#                              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# proc_df_pr_recommendations = df_pr_recommendations[df_pr_recommendations[\"signalID\"] == \"1500\"]\n",
    "\n",
    "# # Create a 2x2 grid for the subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2,\n",
    "#     cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase}\" for phase in proc_df_pr_recommendations[\"phaseNo\"].unique()],\n",
    "#     shared_xaxes=False,\n",
    "#     # shared_yaxes=False,\n",
    "#     vertical_spacing=0.6\n",
    "# )\n",
    "\n",
    "# # Define the color mapping for PR recommendations\n",
    "# colors = {0: \"#27ae60\", 1: \"#cb4335\"}  # Green: PR Not Needed, Red: PR Needed\n",
    "\n",
    "# # Add a heatmap for each phase in the 2x2 layout\n",
    "# phase_nos = proc_df_pr_recommendations[\"phaseNo\"].unique()\n",
    "\n",
    "# for idx, phase_no in enumerate(phase_nos, start=1):\n",
    "#     # Get subplot row and column\n",
    "#     row, col = divmod(idx - 1, 2)\n",
    "#     row += 1\n",
    "#     col += 1\n",
    "\n",
    "#     # Filter data for the current phase\n",
    "#     proc_df_pr_recommendations_phase = (\n",
    "#         proc_df_pr_recommendations[proc_df_pr_recommendations[\"phaseNo\"] == phase_no].iloc[:, 3:]\n",
    "#     )  # Extract hourly binary data\n",
    "    \n",
    "#     pr_recommendations_phase = proc_df_pr_recommendations_phase.values  # Convert to matrix\n",
    "#     x_labels = list(range(24))  # Hours\n",
    "#     y_labels = [\n",
    "#         f\"{row['signalID']} ({row['approachDir']})\" for _, row in proc_df_pr_recommendations[proc_df_pr_recommendations[\"phaseNo\"] == phase_no].iterrows()\n",
    "#     ]\n",
    "\n",
    "#     # Add heatmap trace\n",
    "#     fig.add_trace(\n",
    "#         go.Heatmap(\n",
    "#             z=pr_recommendations_phase,\n",
    "#             x=x_labels,\n",
    "#             y=y_labels,\n",
    "#             colorscale=[[0, \"#27ae60\"], [1, \"#cb4335\"]],  # Define two discrete colors\n",
    "#             showscale=False,  # Disable color bar\n",
    "#             zmin=0,\n",
    "#             zmax=0.5,\n",
    "#         ),\n",
    "#         row=row,\n",
    "#         col=col,\n",
    "#     )\n",
    "\n",
    "# # Add a legend manually\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(size=14, color=\"#27ae60\"),\n",
    "#         name=\"PR Not Recommended\",\n",
    "#     )\n",
    "# )\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(size=14, color=\"#cb4335\"),\n",
    "#         name=\"PR Recommended\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     height=400,\n",
    "#     width=1400,\n",
    "#     # title=\"Hourly Signal PR Recommendations for Each Phase\",\n",
    "#     # yaxis=dict(title=\"Signal ID (Direction)\"),\n",
    "# )\n",
    "\n",
    "# # Update layout with legend font size\n",
    "# fig.update_layout(\n",
    "#     legend=dict(\n",
    "#         orientation=\"h\",  # Horizontal legend\n",
    "#         x=0.5,            # Center horizontally\n",
    "#         y=-0.4,           # Position below the plot\n",
    "#         xanchor=\"center\",\n",
    "#         yanchor=\"top\",\n",
    "#         font=dict(size=14),  # Set font size for the legend items\n",
    "#         # title=dict(text=\"Status\", font=dict(size=14)),  # Title with font size\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Customize x-axis for each subplot\n",
    "# for i in range(1, len(phase_nos) + 1):\n",
    "#     fig.update_xaxes(\n",
    "#         title=\"Hour of Day\",\n",
    "#         tickmode=\"array\",\n",
    "#         tickvals=list(range(24)),\n",
    "#         ticktext=[f\"{hour:02d}:00\" for hour in range(24)],\n",
    "#         row=((i - 1) // 2) + 1,\n",
    "#         col=((i - 1) % 2) + 1,\n",
    "#         tickangle=-90,\n",
    "#         title_font=dict(size=14),\n",
    "#         tickfont=dict(size=14),\n",
    "#     )\n",
    "\n",
    "#     fig.update_yaxes(\n",
    "#         title_font=dict(size=16),\n",
    "#         tickfont=dict(size=14),\n",
    "#     )\n",
    "\n",
    "# # Export the figure as a high-resolution image\n",
    "# fig.write_image(\"../reports/6.2.png\",  width=1400, height=400, scale=2)\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leading Pedestrian Interval and No Right Turn on Red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation of Pedestrian-Vehicle (Right-Turn) Conflict Propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def hourly_aggregate(df):\n",
    "    \"\"\"\n",
    "    Aggregates a DataFrame to calculate hourly statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing cycle-level data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The aggregated DataFrame with hourly statistics.\n",
    "    \"\"\"\n",
    "    # Convert cycleBegin to datetime if it exists\n",
    "    if \"cycleBegin\" in df.columns:\n",
    "        df[\"cycleBegin\"] = pd.to_datetime(df[\"cycleBegin\"])  # Ensure cycleBegin is in datetime format\n",
    "\n",
    "        # Add an hour column for grouping (extract hour from cycleBegin)\n",
    "        df[\"hour\"] = df[\"cycleBegin\"].dt.hour\n",
    "\n",
    "        # # Add isWeekday column to distinguish weekdays (True for Monday-Friday, False for weekends)\n",
    "        # df[\"isWeekday\"] = df[\"cycleBegin\"].dt.dayofweek < 5\n",
    "\n",
    "        # Drop all datetime columns after extracting necessary information\n",
    "        df = df.loc[:, ~df.columns.map(lambda col: pd.api.types.is_datetime64_any_dtype(df[col]))]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    if \"cycleNo\" in df.columns:  # Drop the cycle number column if it exists\n",
    "        df = df.drop(columns=\"cycleNo\")\n",
    "\n",
    "    # Define column categories for aggregation\n",
    "    occupancy_columns = [col for col in df.columns if \"Occupancy\" in col]  # Columns related to occupancy\n",
    "    volume_columns = [col for col in df.columns if \"Volume\" in col]    # Columns related to activity\n",
    "    \n",
    "    duration_columns = [col for col in df.columns if \"Duration\" in col]   # Columns related to duration\n",
    "    activity_columns = [col for col in df.columns if \"Activity\" in col and col not in duration_columns]  # Activity columns excluding durations\n",
    "\n",
    "    df[occupancy_columns + volume_columns + duration_columns + activity_columns] = (\n",
    "        df\n",
    "        [occupancy_columns + volume_columns + duration_columns + activity_columns]\n",
    "        .replace(0, np.nan)\n",
    "    )\n",
    "    \n",
    "    # Prepare the aggregation dictionary\n",
    "    agg_dict = {\n",
    "        **{col: \"mean\" for col in occupancy_columns + duration_columns},  # Calculate the mean for occupancy columns\n",
    "        **{col: \"sum\" for col in activity_columns + volume_columns}       # Calculate the sum for activity and activity columns\n",
    "    }\n",
    "\n",
    "    # Perform the aggregation by grouping by signalID, date, isWeekday, and hour\n",
    "    # df_hourly = df.groupby([\"signalID\", \"date\", \"isWeekday\", \"hour\"]).agg(agg_dict).reset_index()\n",
    "    df_hourly = df.groupby([\"signalID\", \"date\", \"hour\"]).agg(agg_dict).reset_index()\n",
    "\n",
    "    # Calculate additional statistics (max and min) for the grouped data\n",
    "    # agg_dict = {\n",
    "    #     **{col: [\"max\"] for col in df.columns if col not in [\"signalID\", \"date\", \"isWeekday\", \"hour\"]}\n",
    "    # }\n",
    "    # df_hourly_stats = df_hourly.groupby([\"signalID\", \"isWeekday\", \"hour\"]).agg(agg_dict).reset_index()\n",
    "    agg_dict = {\n",
    "        **{col: [\"max\"] for col in df.columns if col not in [\"signalID\", \"date\", \"hour\"]}\n",
    "    }\n",
    "    df_hourly_stats = df_hourly.groupby([\"signalID\", \"hour\"]).agg(agg_dict).reset_index()\n",
    "\n",
    "    # Flatten multi-level column names for readability\n",
    "    df_hourly_stats.columns = [\n",
    "        f\"{col[0]}{col[1].capitalize()}\" if col[1] else col[0] for col in df_hourly_stats.columns.to_flat_index()\n",
    "    ]\n",
    "\n",
    "    # Merge max/min results back with the aggregated hourly data\n",
    "    # df_hourly = df_hourly.merge(\n",
    "    #     df_hourly_stats,\n",
    "    #     on=[\"signalID\", \"isWeekday\", \"hour\"],\n",
    "    #     how=\"left\"\n",
    "    # )\n",
    "\n",
    "    df_hourly = df_hourly.merge(\n",
    "        df_hourly_stats,\n",
    "        on=[\"signalID\", \"hour\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Filter and sort the columns\n",
    "    columns = [\n",
    "        column for column in df_hourly.columns\n",
    "        for param in [\"ActivityDuration\", \"Occupancy\", \"Volume\", \"ActivityPhase\"]\n",
    "        if param in column\n",
    "    ]\n",
    "    columns = sorted(columns, key=lambda x: x.split(\"Phase\")[-1][0])  # Sort columns by phase number\n",
    "\n",
    "    # Reorganize DataFrame columns to make it consistent and intuitive\n",
    "    # df_hourly = df_hourly[[\"signalID\", \"date\", \"isWeekday\", \"hour\"] + columns]\n",
    "    df_hourly = df_hourly[[\"signalID\", \"date\", \"hour\"] + columns]\n",
    "\n",
    "    # Return the aggregated DataFrame\n",
    "    return df_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_diminishing_returns(array, k=0.1):\n",
    "    \"\"\"\n",
    "    Apply diminishing returns using an exponential decay function.\n",
    "\n",
    "    Parameters:\n",
    "        array (np.ndarray): The input array of values.\n",
    "        k (float): The decay rate (higher values result in faster diminishing returns).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Transformed array with diminishing returns applied.\n",
    "    \"\"\"\n",
    "    if not isinstance(array, np.ndarray):\n",
    "        raise ValueError(\"Input must be a NumPy array.\")\n",
    "    \n",
    "    # Apply the diminishing return formula: 1 - e^(-k * x)\n",
    "    return 1 - np.exp(-k * array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(array):\n",
    "    \"\"\"\n",
    "    Apply the sigmoid function to a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "        array (np.ndarray): The input array of values.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Transformed array after applying the sigmoid function.\n",
    "    \"\"\"\n",
    "    if not isinstance(array, np.ndarray):\n",
    "        raise ValueError(\"Input must be a NumPy array.\")\n",
    "    \n",
    "    # Apply the sigmoid formula: 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_conflict_propensity_scores(df: pd.DataFrame, dict_lane_type_weights: dict = None):\n",
    "    \"\"\"\n",
    "    Computes conflict propensity scores for each phase in each row of the DataFrame,\n",
    "    considering all lanes and combining them into phase-level scores using weighted averages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with dynamically generated columns representing phases, e.g.,\n",
    "        'vehicleOccupancyPhase4R', 'vehicleActivityPhase4R', etc.\n",
    "\n",
    "    dict_lane_type_weights : dict, optional\n",
    "        Dictionary mapping lane type suffixes (e.g., 'R', 'TR', 'LTR') to numeric weights.\n",
    "        Example:\n",
    "            {\n",
    "                \"R\":   1.0,    # Right-only\n",
    "                \"TR\":  0.5,    # Through+Right\n",
    "                \"LTR\": 0.33    # Left+Through+Right\n",
    "            }\n",
    "        If None, default weights will be used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with phase-level conflict propensity scores for each row.\n",
    "    \"\"\"\n",
    "    # Assign default lane type weights if none are provided\n",
    "    if dict_lane_type_weights is None:\n",
    "        dict_lane_type_weights = {\n",
    "            \"R\":   1.0,   # Right-only\n",
    "            \"TR\":  0.5,   # Through+Right\n",
    "            \"LTR\": 0.33   # Left+Through+Right\n",
    "        }\n",
    "\n",
    "    # Extract column names and identify unique phases dynamically\n",
    "    columns = df.columns\n",
    "    phase_nos = sorted(\n",
    "        {col.split(\"Phase\")[-1][0] for col in columns if \"Phase\" in col and col.split(\"Phase\")[-1][0].isdigit()}\n",
    "    )\n",
    "\n",
    "    # Initialize a dictionary to store conflict propensity scores for each phase\n",
    "    dict_conflict_propensity_scores = {\n",
    "        f\"turnConflictPropensityPhase{phase_no}\": [] for phase_no in phase_nos\n",
    "    }\n",
    "\n",
    "    # Compute conflict propensity scores for each phase\n",
    "    for phase_no in phase_nos:\n",
    "        weighted_lane_scores = np.zeros(len(df))  # Initialize weighted scores\n",
    "        total_lane_weight = 0  # Initialize total lane weight for normalization\n",
    "\n",
    "        # Calculate total lane weights for normalization\n",
    "        for lane_type, lane_weight in dict_lane_type_weights.items():\n",
    "            volume_col = f\"vehicleVolumePhase{phase_no}{lane_type}\"\n",
    "            occupancy_col = f\"vehicleOccupancyPhase{phase_no}{lane_type}\"\n",
    "\n",
    "            # Only consider lanes that exist in the dataset\n",
    "            if any(col in columns for col in [volume_col, occupancy_col]):\n",
    "                total_lane_weight += lane_weight\n",
    "\n",
    "        # Safeguard: Skip if no valid lanes exist\n",
    "        if total_lane_weight == 0:\n",
    "            continue\n",
    "\n",
    "        # Process each lane type\n",
    "        for lane_type, lane_weight in dict_lane_type_weights.items():\n",
    "            volume_col = f\"vehicleVolumePhase{phase_no}{lane_type}\"\n",
    "            occupancy_col = f\"vehicleOccupancyPhase{phase_no}{lane_type}\"\n",
    "            ped_activity_col = f\"pedestrianActivityPhase{phase_no}\"\n",
    "            ped_activity_duration_col = f\"pedestrianActivityDurationPhase{phase_no}\"\n",
    "\n",
    "            # Ensure pedestrian activity columns exist\n",
    "            if ped_activity_col not in columns or ped_activity_duration_col not in columns:\n",
    "                continue\n",
    "\n",
    "            # Compute vehicle factors\n",
    "            vehicle_factors = np.zeros(len(df))  # Initialize as zeros\n",
    "            if all(col in columns for col in [occupancy_col, volume_col]):\n",
    "                # Calculate occupancy and volume factors if both columns exist\n",
    "                volume_factors = compute_diminishing_returns(df[volume_col].values, k=0.2) * 0.5\n",
    "                occupancy_factors = (df[occupancy_col] / df[ped_activity_duration_col].replace(0, 1)).values * 0.5\n",
    "                vehicle_factors = volume_factors + occupancy_factors\n",
    "            elif occupancy_col in columns:\n",
    "                vehicle_factors = (df[occupancy_col] / df[ped_activity_duration_col].replace(0, 1)).values\n",
    "            elif volume_col in columns:\n",
    "                vehicle_factors = compute_diminishing_returns(df[volume_col].values, k=0.2)\n",
    "\n",
    "            # Compute pedestrian factors\n",
    "            pedestrian_factors = df[ped_activity_col].values\n",
    "\n",
    "            # Compute lane-level scores\n",
    "            lane_scores = compute_diminishing_returns(vehicle_factors * pedestrian_factors, k=0.25)\n",
    "\n",
    "            # Update weighted scores\n",
    "            weighted_lane_scores += (lane_scores * lane_weight) / (lane_weight / total_lane_weight)\n",
    "\n",
    "        # Store phase-level scores\n",
    "        dict_conflict_propensity_scores[f\"turnConflictPropensityPhase{phase_no}\"] = weighted_lane_scores.tolist()\n",
    "\n",
    "    # Return the final DataFrame with conflict propensity scores\n",
    "    return pd.DataFrame(dict_conflict_propensity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_conflict_propensity_thresholds(data, n_clusters=3, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate thresholds for conflict propensity using k-means clustering.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): Array of conflict propensity scores.\n",
    "    n_clusters (int): Number of clusters for k-means.\n",
    "    random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (medium_propensity_threshold, high_propensity_threshold)\n",
    "    \"\"\"\n",
    "    # Reshape data if needed\n",
    "    if len(data.shape) == 1:\n",
    "        data = data.reshape(-1, 1)\n",
    "\n",
    "    # Apply k-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    # Get cluster centroids and sort them\n",
    "    centroids = np.sort(kmeans.cluster_centers_.flatten())\n",
    "\n",
    "    # Define thresholds\n",
    "    medium_propensity_threshold = centroids[1]\n",
    "    high_propensity_threshold = centroids[2]\n",
    "\n",
    "    return medium_propensity_threshold, high_propensity_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def caluate_ci_with_conflict_propensity(signal_id):\n",
    "    df_turn_conflict_propensity_id = load_data(\n",
    "        dirpath=\"../data/production/atspm/fdot_d5/feature_extraction/feature/cycle/pedestrian_traffic/turn_conflict_intensity\",\n",
    "        signal_id=f\"{signal_id}\"\n",
    "    )\n",
    "    \n",
    "    df_turn_conflict_propensity_id[\"hour\"] = df_turn_conflict_propensity_id[\"cycleBegin\"].dt.hour\n",
    "    \n",
    "    columns = (\n",
    "        [col for col in df_turn_conflict_propensity_id.columns if not any(key in col for key in [\"cycle\", \"Begin\", \"End\"])]\n",
    "    )\n",
    "    columns = sorted(columns, key=lambda x: x[0] + x[-1])\n",
    "    \n",
    "    df_turn_conflict_propensity_id = df_turn_conflict_propensity_id[columns]\n",
    "    \n",
    "    df_turn_conflict_propensity_id_hourly = hourly_aggregate(df=df_turn_conflict_propensity_id)\n",
    "    df_turn_conflict_propensity_scores_id_hourly = compute_conflict_propensity_scores(df=df_turn_conflict_propensity_id_hourly)\n",
    "    columns = df_turn_conflict_propensity_scores_id_hourly.columns.tolist()\n",
    "    \n",
    "    df_turn_conflict_propensity_scores_id_hourly[\"signalID\"] = df_turn_conflict_propensity_id_hourly[[\"signalID\"]]\n",
    "    df_turn_conflict_propensity_scores_id_hourly[\"date\"] = df_turn_conflict_propensity_id_hourly[[\"date\"]]\n",
    "    df_turn_conflict_propensity_scores_id_hourly[\"hour\"] = df_turn_conflict_propensity_id_hourly[[\"hour\"]]\n",
    "    \n",
    "    df_turn_conflict_propensity_scores_id_hourly = (\n",
    "        pd.melt(df_turn_conflict_propensity_scores_id_hourly, \n",
    "                id_vars=[\"signalID\", \"date\", \"hour\"], \n",
    "                value_vars=columns, var_name=\"phaseNo\", value_name=\"turnConflictPropensity\")\n",
    "    )\n",
    "    \n",
    "    df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"] = (\n",
    "        df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"]\n",
    "        .str\n",
    "        .extract(r\"Phase(\\d)\")\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    df_turn_conflict_propensity_scores_id_hourly = (\n",
    "        df_turn_conflict_propensity_scores_id_hourly[df_turn_conflict_propensity_scores_id_hourly[\"turnConflictPropensity\"] > 0]\n",
    "    )\n",
    "\n",
    "    # Group by `phaseNo` and `hour` and calculate bootstrap confidence intervals and mean\n",
    "    turn_conflict_propensity_scores_id = []\n",
    "    for (signal_id, hour, phase_no), group in df_turn_conflict_propensity_scores_id_hourly.groupby([\"signalID\", \"hour\", \"phaseNo\"]):\n",
    "        values = group[\"turnConflictPropensity\"].values\n",
    "        lower, upper = bootstrap_ci(values)\n",
    "        mean_value = np.mean(values)  # Calculate the mean\n",
    "        turn_conflict_propensity_scores_id.append(\n",
    "            {\n",
    "                \"signalID\": signal_id, \"hour\": hour, \"phaseNo\": phase_no, \n",
    "                \"turnConflictPropensityAvg\": mean_value, \"lowerCI\": lower, \"upperCI\": upper\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Create a results DataFrame\n",
    "    proc_df_turn_conflict_propensity_scores_id_hourly = pd.DataFrame(turn_conflict_propensity_scores_id)\n",
    "\n",
    "    return df_turn_conflict_propensity_scores_id_hourly, proc_df_turn_conflict_propensity_scores_id_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_turn_conflict_propensity_scores_id_hourly, proc_df_turn_conflict_propensity_scores_id_hourly = (\n",
    "#     caluate_ci_with_conflict_propensity(signal_id=\"1500\")\n",
    "# )\n",
    "\n",
    "# # Filter and reshape data\n",
    "# # turn_conflict_propensity_scores = (\n",
    "# #     df_turn_conflict_propensity_scores_id_hourly\n",
    "# #     .loc[:, \"turnConflictPropensity\"]\n",
    "# #     .values\n",
    "# # )\n",
    "\n",
    "# turn_conflict_propensity_scores = (\n",
    "#     proc_df_turn_conflict_propensity_scores_id_hourly\n",
    "#     .loc[:, \"turnConflictPropensityAvg\"]\n",
    "#     .values\n",
    "# )\n",
    "\n",
    "# # Reshape data for K-Means\n",
    "# turn_conflict_propensity_scores = turn_conflict_propensity_scores.reshape(-1, 1)\n",
    "\n",
    "# # Apply k-means clustering with 3 clusters (low, medium, high propensity)\n",
    "# kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "# kmeans.fit(turn_conflict_propensity_scores)\n",
    "\n",
    "# # Get cluster centroids and labels\n",
    "# centroids = kmeans.cluster_centers_.flatten()\n",
    "# labels = kmeans.labels_\n",
    "\n",
    "# # Assuming turn_conflict_propensity_scores and labels are of equal length\n",
    "# turn_conflict_propensity_scores = turn_conflict_propensity_scores.flatten()  # Ensure 1D array\n",
    "# labels = labels.flatten()  # Ensure 1D array\n",
    "\n",
    "# # Sort centroids to define thresholds logically\n",
    "# centroids = np.sort(centroids)\n",
    "# medium_propensity_centroid = centroids[1]\n",
    "# high_propensity_centroid = centroids[2]\n",
    "\n",
    "# # Display thresholds\n",
    "# print(\"Boundaries for Conflict Propensity:\")\n",
    "# print(f\"Medium Propensity Centroid: {medium_propensity_centroid}\")\n",
    "# print(f\"High Propensity Centroid: {high_propensity_centroid}\")\n",
    "\n",
    "# # Create a scatter plot using Plotly\n",
    "# fig = px.scatter(\n",
    "#     x=turn_conflict_propensity_scores, \n",
    "#     y=labels.astype(int),  # Ensure labels are integers\n",
    "#     color=labels.astype(str),  # Convert labels to string for color grouping\n",
    "#     labels={\"x\": \"Conflict Propensity\", \"y\": \"Cluster Label\", \"color\": \"Cluster\"},\n",
    "#     # title=\"Clustering for Conflict Propensity\"\n",
    "# )\n",
    "\n",
    "# # Add threshold lines\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=[medium_propensity_centroid, medium_propensity_centroid],\n",
    "#     y=[-0.5, max(labels) + 0.5],\n",
    "#     mode=\"lines\",\n",
    "#     line=dict(color=\"orange\", dash=\"dash\"),\n",
    "#     name=f\"Medium Propensity Centroid: {np.round(medium_propensity_centroid, 2)}\"\n",
    "# ))\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=[high_propensity_centroid, high_propensity_centroid],\n",
    "#     y=[-0.5, max(labels) + 0.5],\n",
    "#     mode=\"lines\",\n",
    "#     line=dict(color=\"red\", dash=\"dash\"),\n",
    "#     name=f\"High Propensity Centroid: {np.round(high_propensity_centroid, 2)}\"\n",
    "# ))\n",
    "\n",
    "# # Update layout with integer y-ticks\n",
    "# fig.update_layout(\n",
    "#     height=600,\n",
    "#     width=1400,\n",
    "#     xaxis=dict(title=dict(text=\"Conflict Propensity\", font=dict(size=16))),\n",
    "#     yaxis=dict(\n",
    "#         title=dict(text=\"Cluster Label\", font=dict(size=16)),\n",
    "#         tickmode=\"linear\",  # Ensures consistent tick intervals\n",
    "#         tick0=0,  # Starting tick\n",
    "#         dtick=1,  # Interval between ticks to force integers\n",
    "#     ),\n",
    "#     legend=dict(\n",
    "#         title=dict(text=\"Cluster Legend\", font=dict(size=16, color=\"black\")),  # Bold legend title\n",
    "#         font=dict(size=16, color=\"black\"),  # Bold legend items\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Export the Plotly figure as a high-resolution image\n",
    "# fig.write_image(\"../reports/7.2(a).png\", width=1400, height=600, scale=2)\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_turn_conflict_propensity_scores_id_hourly, proc_df_turn_conflict_propensity_scores_id_hourly = (\n",
    "#     caluate_ci_with_conflict_propensity(signal_id=\"1500\")\n",
    "# )\n",
    "# phase_nos = proc_df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"].unique().tolist()\n",
    "        \n",
    "# # Create a 2x2 grid for subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2, cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase_no}\" for phase_no in phase_nos],\n",
    "#     shared_xaxes=False, shared_yaxes=False,\n",
    "#     vertical_spacing=0.15\n",
    "# )\n",
    "\n",
    "# # Iterate over phases and add traces\n",
    "# row_col_mapping = [(1, 1), (1, 2), (2, 1), (2, 2)]  # Map phases to subplot positions\n",
    "# for idx, (phase_no, (row, col)) in enumerate(zip(phase_nos, row_col_mapping)):\n",
    "#     proc_df_turn_conflict_propensity_scores_id_hourly_phase = (\n",
    "#         proc_df_turn_conflict_propensity_scores_id_hourly[proc_df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"] == phase_no]\n",
    "#     )\n",
    "\n",
    "#     turn_conflict_propensity_scores = (\n",
    "#         proc_df_turn_conflict_propensity_scores_id_hourly\n",
    "#         .loc[:, \"turnConflictPropensityAvg\"]\n",
    "#         .values\n",
    "#     )\n",
    "#     # turn_conflict_propensity_scores = (\n",
    "#     #     # df_turn_conflict_propensity_scores_id_hourly[df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"] == phase_no]\n",
    "#     #     df_turn_conflict_propensity_scores_id_hourly\n",
    "#     #     .loc[:, \"turnConflictPropensity\"]\n",
    "#     #     .values\n",
    "#     # )\n",
    "    \n",
    "#     medium_propensity_threshold, high_propensity_threshold = get_conflict_propensity_thresholds(turn_conflict_propensity_scores)\n",
    "    \n",
    "#     # Calculate error bars\n",
    "#     yerr_lower = proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"turnConflictPropensityAvg\"] - proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"lowerCI\"]\n",
    "#     yerr_upper = proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"upperCI\"] - proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"turnConflictPropensityAvg\"]\n",
    "\n",
    "#     # Add scatter plot with error bars\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"hour\"],\n",
    "#             y=proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"turnConflictPropensityAvg\"],\n",
    "#             error_y=dict(\n",
    "#                 type=\"data\",\n",
    "#                 symmetric=False,\n",
    "#                 array=yerr_upper,\n",
    "#                 arrayminus=yerr_lower,\n",
    "#                 color=\"blue\",\n",
    "#                 thickness=1.5,\n",
    "#                 width=3,\n",
    "#             ),\n",
    "#             mode=\"markers+lines\",\n",
    "#             marker=dict(size=8),\n",
    "#             name=f\"Phase {phase_no}\",\n",
    "#         ),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "#     # Add a horizontal threshold line\n",
    "#     fig.add_shape(\n",
    "#         type=\"line\",\n",
    "#         x0=0,\n",
    "#         x1=23,\n",
    "#         y0=medium_propensity_threshold,\n",
    "#         y1=medium_propensity_threshold,\n",
    "#         line=dict(color=\"orange\", dash=\"dash\"),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "#     # Add a horizontal threshold line\n",
    "#     fig.add_shape(\n",
    "#         type=\"line\",\n",
    "#         x0=0,\n",
    "#         x1=23,\n",
    "#         y0=high_propensity_threshold,\n",
    "#         y1=high_propensity_threshold,\n",
    "#         line=dict(color=\"red\", dash=\"dash\"),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     # title=\"Proportion of Cycles Recommended with PR by Hour with 95% Confidence Intervals\",\n",
    "#     height=800,\n",
    "#     width=1400,\n",
    "#     showlegend=False,\n",
    "#     xaxis_title=\"Hour of Day\",\n",
    "#     yaxis_title=\"Conflict Propensity\",\n",
    "#     # yaxis=dict(range=[0, 1]),  # Set y-axis range\n",
    "#     font=dict(size=14)\n",
    "# )\n",
    "\n",
    "# # Update axis labels for shared x/y axes\n",
    "# fig.update_xaxes(\n",
    "#     title_text=\"Hour of Day\",\n",
    "#     # dtick=1,\n",
    "#     # tickvals=list(range(24)),\n",
    "#     # ticktext=[f\"{hour}:00\" for hour in range(24)],\n",
    "#     # tickformat=\"%H:00\",\n",
    "#     # tickangle=-45,\n",
    "#     # row=2, col=1,  # Shared x-axis title position\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Update y-axes for shared configuration\n",
    "# fig.update_yaxes(\n",
    "#     title_text=\"Conflict Propensity\", \n",
    "#     # range=[0, 1],  # Set y-axis range for all subplots\n",
    "#     # tickvals=np.linspace(0, 1, 11),  # Tick values from 0 to 1 with step 0.1\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Export the Plotly figure as a high-resolution image\n",
    "# fig.write_image(\"../reports/7.2(b).png\", width=1400, height=800, scale=2)\n",
    "\n",
    "# # Show plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identification of Critical Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_with_conflict_propensity(signal_id, metric=\"lpi\"):\n",
    "    dict_hour_with_conflict_propensity_id = {\"signalID\": [], \"approachDir\": [], \"phaseNo\": []}\n",
    "    dict_hour_with_conflict_propensity_id.update(\n",
    "        {str(hour): [] for hour in range(24)}\n",
    "    )\n",
    "    df_config_id = pd.read_csv(f\"../data/interim/atspm/fdot_d5/signal_config/{signal_id}.csv\")\n",
    "    \n",
    "    phase_nos_config = df_config_id[\"phaseNo\"].unique().tolist()\n",
    "    phase_nos_config = [int(phase_no) for phase_no in phase_nos_config if pd.notna(phase_no) and phase_no % 2 == 0]\n",
    "    \n",
    "    try:\n",
    "        df_turn_conflict_propensity_scores_id_hourly, proc_df_turn_conflict_propensity_scores_id_hourly = (\n",
    "            caluate_ci_with_conflict_propensity(signal_id=signal_id)\n",
    "        )\n",
    "        \n",
    "        phase_nos_conflict_propensity = proc_df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"].unique().tolist()\n",
    "        phase_nos_conflict_propensity = [int(phase_no) for phase_no in phase_nos_conflict_propensity]\n",
    "        \n",
    "        sequence = [2, 6, 4, 8]\n",
    "        phase_nos = sorted(list(set(phase_nos_config + phase_nos_conflict_propensity)), key=lambda x: sequence.index(x))\n",
    "    \n",
    "        turn_conflict_propensity_scores = (\n",
    "            proc_df_turn_conflict_propensity_scores_id_hourly\n",
    "            # [proc_df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"] == phase_no]\n",
    "            .loc[:, \"turnConflictPropensityAvg\"]\n",
    "            .values\n",
    "        )\n",
    "        \n",
    "        # turn_conflict_propensity_scores = (\n",
    "        #     df_turn_conflict_propensity_scores_id_hourly\n",
    "        #     # df_turn_conflict_propensity_scores_id_hourly[df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"] == phase_no]\n",
    "        #     .loc[:, \"turnConflictPropensity\"]\n",
    "        #     .values\n",
    "        # )\n",
    "        \n",
    "        medium_propensity_threshold, high_propensity_threshold = get_conflict_propensity_thresholds(turn_conflict_propensity_scores)\n",
    "        \n",
    "        if metric == \"lpi\": # leading pedestrian interval\n",
    "            threshold = medium_propensity_threshold\n",
    "        elif metric == \"nrtor\": # no right turn on red\n",
    "            threshold = high_propensity_threshold\n",
    "        \n",
    "        for phase_no in phase_nos:\n",
    "            dict_hour_with_conflict_propensity_id[\"signalID\"].append(signal_id)\n",
    "    \n",
    "            if phase_no in phase_nos_config:\n",
    "                approach = df_config_id[df_config_id[\"phaseNo\"] == phase_no][\"approach\"].unique()[0]\n",
    "                approach_dir = df_config_id[df_config_id[\"phaseNo\"] == phase_no][\"approach\"].unique()[0].split()[-1][0:-1]\n",
    "            else:\n",
    "                approach = \"NA\"\n",
    "                approach_dir = \"NA\"\n",
    "        \n",
    "            dict_hour_with_conflict_propensity_id[\"approachDir\"].append(approach_dir)\n",
    "            dict_hour_with_conflict_propensity_id[\"phaseNo\"].append(phase_no)\n",
    "    \n",
    "            lane_types = df_config_id[df_config_id[\"phaseNo\"] == phase_no][\"laneType\"].unique().tolist()\n",
    "            lane_types = [lane_type for lane_type in lane_types if \"Right\" in lane_type and len(lane_type.split()) == 1]\n",
    "    \n",
    "            if approach != \"NA\" and len(lane_types) != 0:\n",
    "                stop_bar_distances = (\n",
    "                    df_config_id.query(\"approach == @approach and laneType == @lane_types[-1]\")[\"stopBarDistance\"].values.tolist()\n",
    "                ) \n",
    "                # if stop_bar_distances == []:\n",
    "                #     stop_bar_distances = [np.nan]\n",
    "            else:\n",
    "                stop_bar_distances = (\n",
    "                    df_config_id.query(\"phaseNo == @phase_nos\")[\"stopBarDistance\"].values.tolist()\n",
    "                ) \n",
    "    \n",
    "            if (all(pd.isna(stop_bar_distance) for stop_bar_distance in stop_bar_distances) or approach == \"NA\") and len(lane_types) != 0:\n",
    "                for i in range(24):\n",
    "                    dict_hour_with_conflict_propensity_id[str(i)].append(np.nan)\n",
    "            else:\n",
    "                for i in range(24):\n",
    "                    proc_df_turn_conflict_propensity_scores_id_hourly_phase = (\n",
    "                        proc_df_turn_conflict_propensity_scores_id_hourly\n",
    "                        .query(\"hour == @i & phaseNo == @phase_no\")\n",
    "                    )\n",
    "            \n",
    "                    if len(proc_df_turn_conflict_propensity_scores_id_hourly_phase) == 0:\n",
    "                        dict_hour_with_conflict_propensity_id[str(i)].append(0)\n",
    "                    else:\n",
    "                        proc_df_turn_conflict_propensity_scores_id_hourly_phase = (\n",
    "                            proc_df_turn_conflict_propensity_scores_id_hourly_phase.reset_index(drop=True)\n",
    "                        )\n",
    "                        lower_ci = proc_df_turn_conflict_propensity_scores_id_hourly_phase.loc[0, \"lowerCI\"]\n",
    "                        \n",
    "                        if lower_ci >= threshold:\n",
    "                            dict_hour_with_conflict_propensity_id[str(i)].append(1)\n",
    "                        else:\n",
    "                            dict_hour_with_conflict_propensity_id[str(i)].append(0)\n",
    "    \n",
    "        return pd.DataFrame(dict_hour_with_conflict_propensity_id)\n",
    "    except:\n",
    "        for phase_no in phase_nos_config:\n",
    "            dict_hour_with_conflict_propensity_id[\"signalID\"].append(signal_id)\n",
    "            dict_hour_with_conflict_propensity_id[\"approachDir\"].append(\"NA\")\n",
    "            dict_hour_with_conflict_propensity_id[\"phaseNo\"].append(np.nan)\n",
    "                                                                      \n",
    "            for i in range(24):\n",
    "                dict_hour_with_conflict_propensity_id[str(i)].append(np.nan)\n",
    "\n",
    "        return pd.DataFrame(dict_hour_with_conflict_propensity_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lpi_hour_with_conflict_propensity = pd.DataFrame()\n",
    "\n",
    "# for signal_id in tqdm.tqdm(signal_ids):\n",
    "#     df_lpi_hour_with_conflict_propensity_id = hour_with_conflict_propensity(signal_id=signal_id, metric=\"lpi\")\n",
    "#     df_lpi_hour_with_conflict_propensity = (\n",
    "#         pd.concat([df_lpi_hour_with_conflict_propensity, df_lpi_hour_with_conflict_propensity_id], axis=0, ignore_index=True)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NRTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nrtor_hour_with_conflict_propensity = pd.DataFrame()\n",
    "\n",
    "# for signal_id in tqdm.tqdm(signal_ids):\n",
    "#     df_nrtor_hour_with_conflict_propensity_id = hour_with_conflict_propensity(signal_id=signal_id, metric=\"nrtor\")\n",
    "#     df_nrtor_hour_with_conflict_propensity = (\n",
    "#         pd.concat([df_nrtor_hour_with_conflict_propensity, df_nrtor_hour_with_conflict_propensity_id], axis=0, ignore_index=True)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LPI Recommendation\n",
    "# df_lpi_recommendations = float_to_int(df_lpi_hour_with_conflict_propensity)\n",
    "# df_lpi_recommendations.to_csv(\"../reports/recommendations/lpi_recommendation.csv\", \n",
    "#                               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# proc_df_lpi_recommendations = (\n",
    "#     df_lpi_recommendations[df_lpi_recommendations[\"signalID\"] == \"1500\"]\n",
    "# )\n",
    "\n",
    "# # Create a 2x2 grid for the subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2,\n",
    "#     cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase}\" for phase in proc_df_lpi_recommendations[\"phaseNo\"].unique()],\n",
    "#     shared_xaxes=False,\n",
    "#     # shared_yaxes=False,\n",
    "#     vertical_spacing=0.6\n",
    "# )\n",
    "\n",
    "# # Define the color mapping for LPI recommendations\n",
    "# colors = {0: \"#27ae60\", 1: \"#cb4335\"}  # Green: LPI Not Needed, Red: LPI Needed\n",
    "\n",
    "# # Add a heatmap for each phase in the 2x2 layout\n",
    "# phase_nos = proc_df_lpi_recommendations[\"phaseNo\"].unique()\n",
    "\n",
    "# for idx, phase_no in enumerate(phase_nos, start=1):\n",
    "#     # Get subplot row and column\n",
    "#     row, col = divmod(idx - 1, 2)\n",
    "#     row += 1\n",
    "#     col += 1\n",
    "\n",
    "#     # Filter data for the current phase\n",
    "#     proc_df_lpi_recommendations_phase = (\n",
    "#         proc_df_lpi_recommendations[proc_df_lpi_recommendations[\"phaseNo\"] == phase_no].iloc[:, 3:]\n",
    "#     )  # Extract hourly binary data\n",
    "    \n",
    "#     lpi_recommendations_phase = proc_df_lpi_recommendations_phase.values  # Convert to matrix\n",
    "#     x_labels = list(range(24))  # Hours\n",
    "#     y_labels = [\n",
    "#         f\"{row['signalID']} ({row['approachDir']})\" for _, row in proc_df_lpi_recommendations[proc_df_lpi_recommendations[\"phaseNo\"] == phase_no].iterrows()\n",
    "#     ]\n",
    "\n",
    "#     # Add heatmap trace\n",
    "#     fig.add_trace(\n",
    "#         go.Heatmap(\n",
    "#             z=lpi_recommendations_phase,\n",
    "#             x=x_labels,\n",
    "#             y=y_labels,\n",
    "#             colorscale=[[0, \"#27ae60\"], [1, \"#cb4335\"]],  # Define two discrete colors\n",
    "#             showscale=False,  # Disable color bar\n",
    "#             zmin=0,\n",
    "#             zmax=0.5,\n",
    "#         ),\n",
    "#         row=row,\n",
    "#         col=col,\n",
    "#     )\n",
    "\n",
    "# # Add a legend manually\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(size=14, color=\"#27ae60\"),\n",
    "#         name=\"LPI Not Recommended\",\n",
    "#     )\n",
    "# )\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(size=14, color=\"#cb4335\"),\n",
    "#         name=\"LPI Recommended\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     height=400,\n",
    "#     width=1400,\n",
    "#     # title=\"Hourly Signal PR Recommendations for Each Phase\",\n",
    "#     # yaxis=dict(title=\"Signal ID (Direction)\"),\n",
    "# )\n",
    "\n",
    "# # Update layout with legend font size\n",
    "# fig.update_layout(\n",
    "#     legend=dict(\n",
    "#         orientation=\"h\",  # Horizontal legend\n",
    "#         x=0.5,            # Center horizontally\n",
    "#         y=-0.4,           # Position below the plot\n",
    "#         xanchor=\"center\",\n",
    "#         yanchor=\"top\",\n",
    "#         font=dict(size=14),  # Set font size for the legend items\n",
    "#         # title=dict(text=\"Status\", font=dict(size=14)),  # Title with font size\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Customize x-axis for each subplot\n",
    "# for i in range(1, len(phase_nos) + 1):\n",
    "#     fig.update_xaxes(\n",
    "#         title=\"Hour of Day\",\n",
    "#         tickmode=\"array\",\n",
    "#         tickvals=list(range(24)),\n",
    "#         ticktext=[f\"{hour:02d}:00\" for hour in range(24)],\n",
    "#         row=((i - 1) // 2) + 1,\n",
    "#         col=((i - 1) % 2) + 1,\n",
    "#         tickangle=-90,\n",
    "#         title_font=dict(size=14),\n",
    "#         tickfont=dict(size=14),\n",
    "#     )\n",
    "\n",
    "#     fig.update_yaxes(\n",
    "#         title_font=dict(size=16),\n",
    "#         tickfont=dict(size=14),\n",
    "#     )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NRTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NRTOR Recommendation\n",
    "# df_nrtor_recommendations = float_to_int(df_nrtor_hour_with_conflict_propensity)\n",
    "# df_nrtor_recommendations.to_csv(\"../reports/recommendations/nrtor_recommendation.csv\", \n",
    "#                               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# proc_df_nrtor_recommendations = (\n",
    "#     df_nrtor_recommendations[df_nrtor_recommendations[\"signalID\"] == \"1500\"]\n",
    "# )\n",
    "\n",
    "# # Create a 2x2 grid for the subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2,\n",
    "#     cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase}\" for phase in proc_df_nrtor_recommendations[\"phaseNo\"].unique()],\n",
    "#     shared_xaxes=False,\n",
    "#     # shared_yaxes=False,\n",
    "#     vertical_spacing=0.6\n",
    "# )\n",
    "\n",
    "# # Define the color mapping for NRTOR recommendations\n",
    "# colors = {0: \"#27ae60\", 1: \"#cb4335\"}  # Green: NRTOR Not Needed, Red: NRTOR Needed\n",
    "\n",
    "# # Add a heatmap for each phase in the 2x2 layout\n",
    "# phase_nos = proc_df_nrtor_recommendations[\"phaseNo\"].unique()\n",
    "\n",
    "# for idx, phase_no in enumerate(phase_nos, start=1):\n",
    "#     # Get subplot row and column\n",
    "#     row, col = divmod(idx - 1, 2)\n",
    "#     row += 1\n",
    "#     col += 1\n",
    "\n",
    "#     # Filter data for the current phase\n",
    "#     proc_df_nrtor_recommendations_phase = (\n",
    "#         proc_df_nrtor_recommendations[proc_df_nrtor_recommendations[\"phaseNo\"] == phase_no].iloc[:, 3:]\n",
    "#     )  # Extract hourly binary data\n",
    "    \n",
    "#     nrtor_recommendations_phase = proc_df_nrtor_recommendations_phase.values  # Convert to matrix\n",
    "#     x_labels = list(range(24))  # Hours\n",
    "#     y_labels = [\n",
    "#         f\"{row['signalID']} ({row['approachDir']})\" for _, row in proc_df_nrtor_recommendations[proc_df_nrtor_recommendations[\"phaseNo\"] == phase_no].iterrows()\n",
    "#     ]\n",
    "\n",
    "#     # Add heatmap trace\n",
    "#     fig.add_trace(\n",
    "#         go.Heatmap(\n",
    "#             z=nrtor_recommendations_phase,\n",
    "#             x=x_labels,\n",
    "#             y=y_labels,\n",
    "#             colorscale=[[0, \"#27ae60\"], [1, \"#cb4335\"]],  # Define two discrete colors\n",
    "#             showscale=False,  # Disable color bar\n",
    "#             zmin=0,\n",
    "#             zmax=0.5,\n",
    "#         ),\n",
    "#         row=row,\n",
    "#         col=col,\n",
    "#     )\n",
    "\n",
    "# # Add a legend manually\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(size=14, color=\"#27ae60\"),\n",
    "#         name=\"NRTOR Not Recommended\",\n",
    "#     )\n",
    "# )\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(size=14, color=\"#cb4335\"),\n",
    "#         name=\"NRTOR Recommended\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     height=400,\n",
    "#     width=1400,\n",
    "#     # title=\"Hourly Signal PR Recommendations for Each Phase\",\n",
    "#     # yaxis=dict(title=\"Signal ID (Direction)\"),\n",
    "# )\n",
    "\n",
    "# # Update layout with legend font size\n",
    "# fig.update_layout(\n",
    "#     legend=dict(\n",
    "#         orientation=\"h\",  # Horizontal legend\n",
    "#         x=0.5,            # Center horizontally\n",
    "#         y=-0.4,           # Position below the plot\n",
    "#         xanchor=\"center\",\n",
    "#         yanchor=\"top\",\n",
    "#         font=dict(size=14),  # Set font size for the legend items\n",
    "#         # title=dict(text=\"Status\", font=dict(size=14)),  # Title with font size\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Customize x-axis for each subplot\n",
    "# for i in range(1, len(phase_nos) + 1):\n",
    "#     fig.update_xaxes(\n",
    "#         title=\"Hour of Day\",\n",
    "#         tickmode=\"array\",\n",
    "#         tickvals=list(range(24)),\n",
    "#         ticktext=[f\"{hour:02d}:00\" for hour in range(24)],\n",
    "#         row=((i - 1) // 2) + 1,\n",
    "#         col=((i - 1) % 2) + 1,\n",
    "#         tickangle=-90,\n",
    "#         title_font=dict(size=14),\n",
    "#         tickfont=dict(size=14),\n",
    "#     )\n",
    "\n",
    "#     fig.update_yaxes(\n",
    "#         title_font=dict(size=16),\n",
    "#         tickfont=dict(size=14),\n",
    "#     )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation of Pedestrian-Vehicle (Right-Turn) Conflict Propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def hourly_aggregate(df):\n",
    "    \"\"\"\n",
    "    Aggregates a DataFrame to calculate hourly statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing cycle-level data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The aggregated DataFrame with hourly statistics.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define column categories for aggregation\n",
    "    columns = [col for col in df.columns if any(key in col for key in [\"Duration\", \"Occupancy\"])]  \n",
    "    \n",
    "    df[columns] = df[columns].replace(0, np.nan)\n",
    "    \n",
    "    # Prepare the aggregation dictionary\n",
    "    agg_dict = {\n",
    "        **{col: \"mean\" for col in columns}  \n",
    "    }\n",
    "\n",
    "    # Perform the aggregation by grouping by signalID, date, and hour\n",
    "    df_hourly = df.groupby([\"signalID\", \"date\", \"hour\"]).agg(agg_dict).reset_index()\n",
    "\n",
    "    # Filter and sort the columns\n",
    "    columns = [\n",
    "        column for column in df_hourly.columns\n",
    "        for param in [\"Duration\", \"Occupancy\"]\n",
    "        if param in column\n",
    "    ]\n",
    "    columns = sorted(columns, key=lambda x: x.split(\"Phase\")[-1][0])  # Sort columns by phase number\n",
    "\n",
    "    # Reorganize DataFrame columns to make it consistent and intuitive\n",
    "    df_hourly = df_hourly[[\"signalID\", \"date\", \"hour\"] + columns]\n",
    "\n",
    "    # Return the aggregated DataFrame\n",
    "    return df_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_diminishing_returns(array, k=0.1):\n",
    "    \"\"\"\n",
    "    Apply diminishing returns using an exponential decay function, returning 0 for NaN values.\n",
    "\n",
    "    Parameters:\n",
    "        array (np.ndarray): The input array of values.\n",
    "        k (float): The decay rate (higher values result in faster diminishing returns).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Transformed array with diminishing returns applied, with NaN values returning 0.\n",
    "    \"\"\"\n",
    "    if not isinstance(array, np.ndarray):\n",
    "        raise ValueError(\"Input must be a NumPy array.\")\n",
    "    \n",
    "    # Replace NaN values with 0 before computing diminishing returns\n",
    "    array = np.nan_to_num(array, nan=0)\n",
    "    \n",
    "    # Apply the diminishing return formula: 1 - e^(-k * x)\n",
    "    return 1 - np.exp(-k * array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_sigmoid_scaling(array, k=0.1, C=None):\n",
    "    \"\"\"\n",
    "    Apply sigmoid transformation to scale values between 0 and 1.\n",
    "\n",
    "    Parameters:\n",
    "        array (np.ndarray): The input array of values.\n",
    "        k (float): The scaling factor controlling sensitivity of the sigmoid function.\n",
    "        C (float, optional): The threshold parameter. \n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Transformed array with sigmoid scaling applied.\n",
    "    \"\"\"\n",
    "    if not isinstance(array, np.ndarray):\n",
    "        raise ValueError(\"Input must be a NumPy array.\")\n",
    "    \n",
    "    # Replace NaN values with 0 before computing sigmoid scaling\n",
    "    array = np.nan_to_num(array, nan=0)\n",
    "\n",
    "    # Set threshold C if not provided\n",
    "    if C is None:\n",
    "        # C = np.median(array)\n",
    "        C = np.percentile(array, 1)  # Use 75th percentile instead of median\n",
    "\n",
    "    # Apply sigmoid function: 1 / (1 + e^(-k * (x - C)))\n",
    "    return 1 / (1 + np.exp(-k * (array - C)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conflict_propensity_scores(df: pd.DataFrame, dict_lane_type_weights: dict = None, k: float = None):\n",
    "    \"\"\"\n",
    "    Computes conflict propensity scores for each phase in each row of the DataFrame,\n",
    "    considering all lanes and combining them into phase-level scores using weighted averages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with dynamically generated columns representing phases, e.g.,\n",
    "        'vehicleOccupancyPhase4R', 'vehicleActivityPhase4R', etc.\n",
    "\n",
    "    dict_lane_type_weights : dict, optional\n",
    "        Dictionary mapping lane type suffixes (e.g., 'R', 'TR', 'LTR') to numeric weights.\n",
    "        Example:\n",
    "            {\n",
    "                \"R\":   1.0,    # Right-only\n",
    "                \"TR\":  0.5,    # Through+Right\n",
    "                \"LTR\": 0.33    # Left+Through+Right\n",
    "            }\n",
    "        If None, default weights will be used.\n",
    "        \n",
    "    k (float): The decay rate (higher values result in faster diminishing returns).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with phase-level conflict propensity scores for each row.\n",
    "    \"\"\"\n",
    "    # df_turn_conflict_propensity_hourly = concat_df()\n",
    "    \n",
    "    # Assign default lane type weights if none are provided\n",
    "    if dict_lane_type_weights is None:\n",
    "        dict_lane_type_weights = {\n",
    "            \"R\":   1.0,   # Right-only\n",
    "            \"TR\":  0.5,   # Through+Right\n",
    "            \"LTR\": 0.33   # Left+Through+Right\n",
    "        }\n",
    "\n",
    "    # Extract column names and identify unique phases dynamically\n",
    "    columns = df.columns\n",
    "    phase_nos = sorted(\n",
    "        {int(col.split(\"Phase\")[-1][0]) for col in columns if \"Phase\" in col and col.split(\"Phase\")[-1][0].isdigit()}\n",
    "    )\n",
    "    phase_nos = [phase_no for phase_no in phase_nos if phase_no in [2, 4, 6, 8]]\n",
    "\n",
    "    # Initialize a dictionary to store conflict propensity scores for each phase\n",
    "    dict_conflict_propensity_scores = {\n",
    "        f\"turnConflictPropensityPhase{phase_no}\": [] for phase_no in phase_nos\n",
    "    }\n",
    "\n",
    "    # Compute conflict propensity scores for each phase\n",
    "    for phase_no in phase_nos:\n",
    "        weighted_lane_scores = np.zeros(len(df))  # Initialize weighted scores\n",
    "        total_lane_weight = 0  # Initialize total lane weight for normalization\n",
    "\n",
    "        # Calculate total lane weights for normalization\n",
    "        for lane_type, lane_weight in dict_lane_type_weights.items():\n",
    "            vehicle_occupancy_col = f\"vehicleOccupancyPhase{phase_no}{lane_type}\"\n",
    "\n",
    "            # Only consider lanes that exist in the dataset\n",
    "            if any(col in columns for col in [vehicle_occupancy_col]):\n",
    "                total_lane_weight += lane_weight\n",
    "\n",
    "        # Safeguard: Skip if no valid lanes exist\n",
    "        if total_lane_weight == 0:\n",
    "            continue\n",
    "\n",
    "        # Process each lane type\n",
    "        for lane_type, lane_weight in dict_lane_type_weights.items():\n",
    "            vehicle_occupancy_col = f\"vehicleOccupancyPhase{phase_no}{lane_type}\"\n",
    "            pedestrian_activity_duration_col = f\"pedestrianActivityDurationPhase{phase_no}\"\n",
    "\n",
    "            # Ensure pedestrian activity columns exist\n",
    "            if pedestrian_activity_duration_col not in columns or vehicle_occupancy_col not in columns:\n",
    "                continue\n",
    "\n",
    "            # Calculate exposure \n",
    "            pedestrian_activity_durations = df[pedestrian_activity_duration_col].values\n",
    "            vehicle_occupancies = df[vehicle_occupancy_col].values\n",
    "\n",
    "            # exposures = pedestrian_activity_durations * vehicle_occupancies\n",
    "            # exposures = np.sqrt(pedestrian_activity_durations * vehicle_occupancies)\n",
    "            exposures = (\n",
    "                (2 * (pedestrian_activity_durations * vehicle_occupancies)) / (pedestrian_activity_durations + vehicle_occupancies)\n",
    "            )\n",
    "\n",
    "            # # Determine k\n",
    "            # all_exposures = (\n",
    "            #     df_turn_conflict_propensity_hourly[pedestrian_activity_duration_col].values * \n",
    "            #     df_turn_conflict_propensity_hourly[vehicle_occupancy_col].values\n",
    "            # )\n",
    "\n",
    "            # k = 1 / np.nanmax(all_exposures) if np.nanmax(all_exposures) > 0 else 0\n",
    "\n",
    "            # Calculate dimishing return\n",
    "            diminishing_returns = compute_diminishing_returns(array=exposures, k=k)\n",
    "\n",
    "            # Calculate lane-level scores\n",
    "            lane_scores = lane_weight * diminishing_returns\n",
    "            # lane_scores = diminishing_returns\n",
    "\n",
    "            # # Calculate dimishing return\n",
    "            # sigmoid_scaling = compute_sigmoid_scaling(array=exposures, k=k)\n",
    "\n",
    "            # # Calculate lane-level scores\n",
    "            # lane_scores = lane_weight * sigmoid_scaling\n",
    "\n",
    "            # Compute weighted lane-level scores\n",
    "            weighted_lane_scores = lane_scores * (lane_weight / total_lane_weight)\n",
    "\n",
    "        # Store phase-level scores\n",
    "        dict_conflict_propensity_scores[f\"turnConflictPropensityPhase{phase_no}\"] = weighted_lane_scores.tolist()\n",
    "        # dict_conflict_propensity_scores[f\"turnConflictPropensityPhase{phase_no}\"] = exposures.tolist()\n",
    "\n",
    "    # Return the final DataFrame with conflict propensity scores\n",
    "    return pd.DataFrame(dict_conflict_propensity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def caluate_ci_with_conflict_propensity(signal_id: str, k: float = None):\n",
    "    df_turn_conflict_propensity_id = load_data(\n",
    "        dirpath=\"../data/production/atspm/fdot_d5/feature_extraction/feature/cycle/pedestrian_traffic/turn_conflict_intensity\",\n",
    "        signal_id=f\"{signal_id}\"\n",
    "    )\n",
    "    df_turn_conflict_propensity_id[\"hour\"] = df_turn_conflict_propensity_id[\"cycleBegin\"].dt.hour\n",
    "    \n",
    "    columns = (\n",
    "        [col for col in df_turn_conflict_propensity_id.columns if any(key in col for key in [\"signalID\", \"date\", \"hour\", \"Duration\", \"Occupancy\"])]\n",
    "    )\n",
    "    columns = sorted(columns, key=lambda x: x[0] + x[-1])\n",
    "    \n",
    "    df_turn_conflict_propensity_id = df_turn_conflict_propensity_id[columns]\n",
    "    \n",
    "    df_turn_conflict_propensity_id_hourly = hourly_aggregate(df=df_turn_conflict_propensity_id)\n",
    "    df_turn_conflict_propensity_scores_id_hourly = (\n",
    "        compute_conflict_propensity_scores(df=df_turn_conflict_propensity_id_hourly, k=k)\n",
    "    )\n",
    "    columns = df_turn_conflict_propensity_scores_id_hourly.columns.tolist()\n",
    "    \n",
    "    df_turn_conflict_propensity_scores_id_hourly[\"signalID\"] = df_turn_conflict_propensity_id_hourly[[\"signalID\"]]\n",
    "    df_turn_conflict_propensity_scores_id_hourly[\"date\"] = df_turn_conflict_propensity_id_hourly[[\"date\"]]\n",
    "    df_turn_conflict_propensity_scores_id_hourly[\"hour\"] = df_turn_conflict_propensity_id_hourly[[\"hour\"]]\n",
    "    \n",
    "    df_turn_conflict_propensity_scores_id_hourly = (\n",
    "        pd.melt(df_turn_conflict_propensity_scores_id_hourly, \n",
    "                id_vars=[\"signalID\", \"date\", \"hour\"], \n",
    "                value_vars=columns, var_name=\"phaseNo\", value_name=\"turnConflictPropensity\")\n",
    "    )\n",
    "    \n",
    "    df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"] = (\n",
    "        df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"]\n",
    "        .str\n",
    "        .extract(r\"Phase(\\d)\")\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    df_turn_conflict_propensity_scores_id_hourly = (\n",
    "        df_turn_conflict_propensity_scores_id_hourly[df_turn_conflict_propensity_scores_id_hourly[\"turnConflictPropensity\"] > 0]\n",
    "    )\n",
    "\n",
    "    # Group by `phaseNo` and `hour` and calculate bootstrap confidence intervals and mean\n",
    "    turn_conflict_propensity_scores_id = []\n",
    "    for (signal_id, hour, phase_no), group in df_turn_conflict_propensity_scores_id_hourly.groupby([\"signalID\", \"hour\", \"phaseNo\"]):\n",
    "        values = group[\"turnConflictPropensity\"].values\n",
    "        lower, upper = bootstrap_ci(values)\n",
    "        mean_value = np.mean(values)  # Calculate the mean\n",
    "        turn_conflict_propensity_scores_id.append(\n",
    "            {\n",
    "                \"signalID\": signal_id, \"hour\": hour, \"phaseNo\": phase_no, \n",
    "                \"turnConflictPropensityAvg\": mean_value, \"lowerCI\": lower, \"upperCI\": upper\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Create a results DataFrame\n",
    "    proc_df_turn_conflict_propensity_scores_id_hourly = pd.DataFrame(turn_conflict_propensity_scores_id)\n",
    "\n",
    "    return df_turn_conflict_propensity_scores_id_hourly, proc_df_turn_conflict_propensity_scores_id_hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identification of Critical Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def concat_df():\n",
    "#     df_turn_conflict_propensity_hourly = pd.DataFrame()\n",
    "    \n",
    "#     for signal_id in signal_ids:\n",
    "#         try:\n",
    "#             df_turn_conflict_propensity_id = load_data(\n",
    "#                 dirpath=\"../data/production/atspm/fdot_d5/feature_extraction/feature/cycle/pedestrian_traffic/turn_conflict_intensity\",\n",
    "#                 signal_id=f\"{signal_id}\"\n",
    "#             )\n",
    "#             df_turn_conflict_propensity_id[\"hour\"] = df_turn_conflict_propensity_id[\"cycleBegin\"].dt.hour\n",
    "            \n",
    "#             columns = (\n",
    "#                 [col for col in df_turn_conflict_propensity_id.columns if any(key in col for key in [\"signalID\", \"date\", \"hour\", \"Duration\", \"Occupancy\"])]\n",
    "#             )\n",
    "#             columns = sorted(columns, key=lambda x: x[0] + x[-1])\n",
    "            \n",
    "#             df_turn_conflict_propensity_id = df_turn_conflict_propensity_id[columns]\n",
    "#             df_turn_conflict_propensity_id_hourly = hourly_aggregate(df=df_turn_conflict_propensity_id)\n",
    "\n",
    "#             df_turn_conflict_propensity_hourly = pd.concat(\n",
    "#                 [df_turn_conflict_propensity_hourly, df_turn_conflict_propensity_id_hourly], \n",
    "#                 axis=0, ignore_index=True\n",
    "#             )\n",
    "#         except:\n",
    "#             print(signal_id)\n",
    "\n",
    "#     return df_turn_conflict_propensity_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_turn_conflict_propensity_scores_id_hourly, proc_df_turn_conflict_propensity_scores_id_hourly = (\n",
    "#     caluate_ci_with_conflict_propensity(signal_id=\"1500\", k=0.01)\n",
    "# )\n",
    "# phase_nos = proc_df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"].unique().tolist()\n",
    "# phase_nos = [2, 6, 4, 8]\n",
    "        \n",
    "# # Create a 2x2 grid for subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2, cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase_no}\" for phase_no in phase_nos],\n",
    "#     shared_xaxes=False, shared_yaxes=False,\n",
    "#     vertical_spacing=0.15\n",
    "# )\n",
    "\n",
    "# # Iterate over phases and add traces\n",
    "# row_col_mapping = [(1, 1), (1, 2), (2, 1), (2, 2)]  # Map phases to subplot positions\n",
    "# for idx, (phase_no, (row, col)) in enumerate(zip(phase_nos, row_col_mapping)):\n",
    "#     proc_df_turn_conflict_propensity_scores_id_hourly_phase = (\n",
    "#         proc_df_turn_conflict_propensity_scores_id_hourly[proc_df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"] == phase_no]\n",
    "#     )\n",
    "\n",
    "#     turn_conflict_propensity_scores = (\n",
    "#         proc_df_turn_conflict_propensity_scores_id_hourly\n",
    "#         .loc[:, \"turnConflictPropensityAvg\"]\n",
    "#         .values\n",
    "#     )\n",
    "#     # turn_conflict_propensity_scores = (\n",
    "#     #     # df_turn_conflict_propensity_scores_id_hourly[df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"] == phase_no]\n",
    "#     #     df_turn_conflict_propensity_scores_id_hourly\n",
    "#     #     .loc[:, \"turnConflictPropensity\"]\n",
    "#     #     .values\n",
    "#     # )\n",
    "    \n",
    "#     # Calculate error bars\n",
    "#     yerr_lower = proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"turnConflictPropensityAvg\"] - proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"lowerCI\"]\n",
    "#     yerr_upper = proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"upperCI\"] - proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"turnConflictPropensityAvg\"]\n",
    "\n",
    "#     # Add scatter plot with error bars\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"hour\"],\n",
    "#             y=proc_df_turn_conflict_propensity_scores_id_hourly_phase[\"turnConflictPropensityAvg\"],\n",
    "#             error_y=dict(\n",
    "#                 type=\"data\",\n",
    "#                 symmetric=False,\n",
    "#                 array=yerr_upper,\n",
    "#                 arrayminus=yerr_lower,\n",
    "#                 color=\"blue\",\n",
    "#                 thickness=1.5,\n",
    "#                 width=3,\n",
    "#             ),\n",
    "#             mode=\"markers+lines\",\n",
    "#             marker=dict(size=8),\n",
    "#             name=f\"Phase {phase_no}\",\n",
    "#         ),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "#     # Add a horizontal threshold line\n",
    "#     fig.add_shape(\n",
    "#         type=\"line\",\n",
    "#         x0=0,\n",
    "#         x1=23,\n",
    "#         y0=0.5,\n",
    "#         y1=0.5,\n",
    "#         line=dict(color=\"red\", dash=\"dash\"),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     # title=\"Proportion of Cycles Recommended with PR by Hour with 95% Confidence Intervals\",\n",
    "#     height=800,\n",
    "#     width=1400,\n",
    "#     showlegend=False,\n",
    "#     xaxis_title=\"Hour of Day\",\n",
    "#     yaxis_title=\"Conflict Propensity\",\n",
    "#     # yaxis=dict(range=[0, 1]),  # Set y-axis range\n",
    "#     font=dict(size=14)\n",
    "# )\n",
    "\n",
    "# # Update axis labels for shared x/y axes\n",
    "# fig.update_xaxes(\n",
    "#     title_text=\"Hour of Day\",\n",
    "#     # dtick=1,\n",
    "#     # tickvals=list(range(24)),\n",
    "#     # ticktext=[f\"{hour}:00\" for hour in range(24)],\n",
    "#     # tickformat=\"%H:00\",\n",
    "#     # tickangle=-45,\n",
    "#     # row=2, col=1,  # Shared x-axis title position\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Update y-axes for shared configuration\n",
    "# fig.update_yaxes(\n",
    "#     title_text=\"Conflict Propensity\", \n",
    "#     range=[0, 1],  # Set y-axis range for all subplots\n",
    "#     # tickvals=np.linspace(0, 1, 11),  # Tick values from 0 to 1 with step 0.1\n",
    "#     title_font=dict(size=16),\n",
    "#     tickfont=dict(size=16),\n",
    "# )\n",
    "\n",
    "# # Export the Plotly figure as a high-resolution image\n",
    "# fig.write_image(\"../reports/7.1.png\", width=1400, height=800, scale=2)\n",
    "\n",
    "# # Show plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def critical_hour_with_conflict_propensity(signal_id, threshold, k):\n",
    "    dict_hour_with_conflict_propensity_id = {\"signalID\": [], \"approachDir\": [], \"phaseNo\": []}\n",
    "    dict_hour_with_conflict_propensity_id.update(\n",
    "        {str(hour): [] for hour in range(24)}\n",
    "    )\n",
    "    df_config_id = pd.read_csv(f\"../data/interim/atspm/fdot_d5/signal_config/{signal_id}.csv\")\n",
    "    \n",
    "    phase_nos_config = df_config_id[\"phaseNo\"].unique().tolist()\n",
    "    phase_nos_config = [int(phase_no) for phase_no in phase_nos_config if pd.notna(phase_no) and phase_no % 2 == 0]\n",
    "    \n",
    "    try:\n",
    "        _, proc_df_turn_conflict_propensity_scores_id_hourly = (\n",
    "            caluate_ci_with_conflict_propensity(signal_id=signal_id, k=k)\n",
    "        )\n",
    "\n",
    "        proc_df_turn_conflict_propensity_scores_id_hourly = (\n",
    "            proc_df_turn_conflict_propensity_scores_id_hourly\n",
    "            [proc_df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"].isin([2, 4, 6, 8])]\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        \n",
    "        phase_nos_conflict_propensity = proc_df_turn_conflict_propensity_scores_id_hourly[\"phaseNo\"].unique().tolist()\n",
    "        phase_nos_conflict_propensity = [int(phase_no) for phase_no in phase_nos_conflict_propensity]\n",
    "        \n",
    "        sequence = [2, 6, 4, 8]\n",
    "        phase_nos = sorted(list(set(phase_nos_config + phase_nos_conflict_propensity)), key=lambda x: sequence.index(x))\n",
    "        \n",
    "        for phase_no in phase_nos:\n",
    "            dict_hour_with_conflict_propensity_id[\"signalID\"].append(signal_id)\n",
    "    \n",
    "            if phase_no in phase_nos_config:\n",
    "                approach = df_config_id[df_config_id[\"phaseNo\"] == phase_no][\"approach\"].unique()[0]\n",
    "                approach_dir = df_config_id[df_config_id[\"phaseNo\"] == phase_no][\"approach\"].unique()[0].split()[-1][0:-1]\n",
    "            else:\n",
    "                approach = \"NA\"\n",
    "                approach_dir = \"NA\"\n",
    "        \n",
    "            dict_hour_with_conflict_propensity_id[\"approachDir\"].append(approach_dir)\n",
    "            dict_hour_with_conflict_propensity_id[\"phaseNo\"].append(phase_no)\n",
    "    \n",
    "            lane_types = df_config_id[df_config_id[\"phaseNo\"] == phase_no][\"laneType\"].unique().tolist()\n",
    "            lane_types = [lane_type for lane_type in lane_types if \"Right\" in lane_type and len(lane_type.split()) == 1]\n",
    "    \n",
    "            if approach != \"NA\" and len(lane_types) != 0:\n",
    "                stop_bar_distances = (\n",
    "                    df_config_id.query(\"approach == @approach and laneType == @lane_types[-1]\")[\"stopBarDistance\"].values.tolist()\n",
    "                ) \n",
    "                # if stop_bar_distances == []:\n",
    "                #     stop_bar_distances = [np.nan]\n",
    "            else:\n",
    "                stop_bar_distances = (\n",
    "                    df_config_id.query(\"phaseNo == @phase_nos\")[\"stopBarDistance\"].values.tolist()\n",
    "                ) \n",
    "    \n",
    "            if (all(pd.isna(stop_bar_distance) for stop_bar_distance in stop_bar_distances) or approach == \"NA\") and len(lane_types) != 0:\n",
    "                for i in range(24):\n",
    "                    dict_hour_with_conflict_propensity_id[str(i)].append(np.nan)\n",
    "            else:\n",
    "                for i in range(24):\n",
    "                    proc_df_turn_conflict_propensity_scores_id_hourly_phase = (\n",
    "                        proc_df_turn_conflict_propensity_scores_id_hourly\n",
    "                        .query(\"hour == @i & phaseNo == @phase_no\")\n",
    "                    )\n",
    "            \n",
    "                    if len(proc_df_turn_conflict_propensity_scores_id_hourly_phase) == 0:\n",
    "                        dict_hour_with_conflict_propensity_id[str(i)].append(0)\n",
    "                    else:\n",
    "                        proc_df_turn_conflict_propensity_scores_id_hourly_phase = (\n",
    "                            proc_df_turn_conflict_propensity_scores_id_hourly_phase.reset_index(drop=True)\n",
    "                        )\n",
    "                        lower_ci = proc_df_turn_conflict_propensity_scores_id_hourly_phase.loc[0, \"lowerCI\"]\n",
    "                        \n",
    "                        if lower_ci >= threshold:\n",
    "                            dict_hour_with_conflict_propensity_id[str(i)].append(1)\n",
    "                        else:\n",
    "                            dict_hour_with_conflict_propensity_id[str(i)].append(0)\n",
    "    \n",
    "        return pd.DataFrame(dict_hour_with_conflict_propensity_id)\n",
    "    except:\n",
    "        for phase_no in phase_nos_config:\n",
    "            dict_hour_with_conflict_propensity_id[\"signalID\"].append(signal_id)\n",
    "            dict_hour_with_conflict_propensity_id[\"approachDir\"].append(\"NA\")\n",
    "            dict_hour_with_conflict_propensity_id[\"phaseNo\"].append(np.nan)\n",
    "                                                                      \n",
    "            for i in range(24):\n",
    "                dict_hour_with_conflict_propensity_id[str(i)].append(np.nan)\n",
    "\n",
    "        return pd.DataFrame(dict_hour_with_conflict_propensity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 19/19 [00:15<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "df_critical_hour_with_conflict_propensity = pd.DataFrame()\n",
    "\n",
    "for signal_id in tqdm.tqdm(signal_ids):\n",
    "    df_critical_hour_with_conflict_propensity_id = critical_hour_with_conflict_propensity(\n",
    "        signal_id=signal_id, threshold=0.5, k=0.025\n",
    "    )\n",
    "    df_critical_hour_with_conflict_propensity = (\n",
    "        pd.concat(\n",
    "            [df_critical_hour_with_conflict_propensity, df_critical_hour_with_conflict_propensity_id], \n",
    "            axis=0, ignore_index=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lpi_nrtor_recommendations = float_to_int(df_critical_hour_with_conflict_propensity)\n",
    "df_lpi_nrtor_recommendations.to_csv(\"../reports/recommendations/lpi_nrtor_recommendations.csv\", \n",
    "                                    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# proc_df_lpi_nrtor_recommendations = df_lpi_nrtor_recommendations[df_lpi_nrtor_recommendations[\"signalID\"] == \"1500\"]\n",
    "\n",
    "# # Create a 2x2 grid for the subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2,\n",
    "#     cols=2,\n",
    "#     subplot_titles=[f\"Phase {phase}\" for phase in proc_df_lpi_nrtor_recommendations[\"phaseNo\"].unique()],\n",
    "#     shared_xaxes=False,\n",
    "#     # shared_yaxes=False,\n",
    "#     vertical_spacing=0.6\n",
    "# )\n",
    "\n",
    "# # Define the color mapping for PR recommendations\n",
    "# colors = {0: \"#27ae60\", 1: \"#cb4335\"}  # Green: PR Not Needed, Red: PR Needed\n",
    "\n",
    "# # Add a heatmap for each phase in the 2x2 layout\n",
    "# phase_nos = proc_df_lpi_nrtor_recommendations[\"phaseNo\"].unique()\n",
    "\n",
    "# for idx, phase_no in enumerate(phase_nos, start=1):\n",
    "#     # Get subplot row and column\n",
    "#     row, col = divmod(idx - 1, 2)\n",
    "#     row += 1\n",
    "#     col += 1\n",
    "\n",
    "#     # Filter data for the current phase\n",
    "#     proc_df_lpi_nrtor_recommendations_phase = (\n",
    "#         proc_df_lpi_nrtor_recommendations[proc_df_lpi_nrtor_recommendations[\"phaseNo\"] == phase_no].iloc[:, 3:]\n",
    "#     )  # Extract hourly binary data\n",
    "    \n",
    "#     lpi_nrtor_recommendations_phase = proc_df_lpi_nrtor_recommendations_phase.values  # Convert to matrix\n",
    "#     x_labels = list(range(24))  # Hours\n",
    "#     y_labels = [\n",
    "#         f\"{row['signalID']} ({row['approachDir']})\" for _, row in proc_df_lpi_nrtor_recommendations[proc_df_lpi_nrtor_recommendations[\"phaseNo\"] == phase_no].iterrows()\n",
    "#     ]\n",
    "\n",
    "#     # Add heatmap trace\n",
    "#     fig.add_trace(\n",
    "#         go.Heatmap(\n",
    "#             z=lpi_nrtor_recommendations_phase,\n",
    "#             x=x_labels,\n",
    "#             y=y_labels,\n",
    "#             colorscale=[[0, \"#27ae60\"], [1, \"#cb4335\"]],  # Define two discrete colors\n",
    "#             showscale=False,  # Disable color bar\n",
    "#             zmin=0,\n",
    "#             zmax=0.5,\n",
    "#         ),\n",
    "#         row=row,\n",
    "#         col=col,\n",
    "#     )\n",
    "\n",
    "# # Add a legend manually\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(size=14, color=\"#27ae60\"),\n",
    "#         name=\"NRTOR Not Recommended\",\n",
    "#     )\n",
    "# )\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(size=14, color=\"#cb4335\"),\n",
    "#         name=\"NRTOR Recommended\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     height=400,\n",
    "#     width=1400,\n",
    "#     # title=\"Hourly Signal PR Recommendations for Each Phase\",\n",
    "#     # yaxis=dict(title=\"Signal ID (Direction)\"),\n",
    "# )\n",
    "\n",
    "# # Update layout with legend font size\n",
    "# fig.update_layout(\n",
    "#     legend=dict(\n",
    "#         orientation=\"h\",  # Horizontal legend\n",
    "#         x=0.5,            # Center horizontally\n",
    "#         y=-0.4,           # Position below the plot\n",
    "#         xanchor=\"center\",\n",
    "#         yanchor=\"top\",\n",
    "#         font=dict(size=14),  # Set font size for the legend items\n",
    "#         # title=dict(text=\"Status\", font=dict(size=14)),  # Title with font size\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Customize x-axis for each subplot\n",
    "# for i in range(1, len(phase_nos) + 1):\n",
    "#     fig.update_xaxes(\n",
    "#         title=\"Hour of Day\",\n",
    "#         tickmode=\"array\",\n",
    "#         tickvals=list(range(24)),\n",
    "#         ticktext=[f\"{hour:02d}:00\" for hour in range(24)],\n",
    "#         row=((i - 1) // 2) + 1,\n",
    "#         col=((i - 1) % 2) + 1,\n",
    "#         tickangle=-90,\n",
    "#         title_font=dict(size=14),\n",
    "#         tickfont=dict(size=14),\n",
    "#     )\n",
    "\n",
    "#     fig.update_yaxes(\n",
    "#         title_font=dict(size=16),\n",
    "#         tickfont=dict(size=14),\n",
    "#     )\n",
    "\n",
    "# # Export the figure as a high-resolution image\n",
    "# fig.write_image(\"../reports/7.2(b).png\",  width=1400, height=400, scale=2)\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
